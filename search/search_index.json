{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"HPC Multibench","text":"<p>A Swiss army knife for comparing programs on HPC resources.</p> <p><code>hpc-multibench</code> is a Python tool to run, aggregate, and analyse metrics about HPC batch compute jobs via Slurm from a convenient YAML format.</p>"},{"location":"index.html#killer-features","title":"Killer features","text":"<ul> <li> Define experiments from a convenient YAML file</li> <li> Support for zero effort re-runs of experiments, with aggregation for       uncertainty calculations and error bars</li> <li> Simple metric extraction and graph plotting from run results, including       line, bar and roofline plots</li> <li> Rendered entirely in the terminal \u2013 including graph plotting capabilities;       no need to set up X-forwarding!</li> </ul>"},{"location":"index.html#system-requirements","title":"System requirements","text":"<p>Due to the libraries for parsing the YAML schema, a Python installation of version greater than 3.10 is required.</p> <p>Since this tool uses Slurm to dispatch, the system must have Slurm installed in order to use the <code>record</code> functionality to dispatch runs. However, it can be used on systems without Slurm to view and analyse existing run files, using the <code>report</code> functionality.</p>"},{"location":"contributing.html","title":"Contributing","text":"<p>This project is open for contributions! You can do this by making a pull request.</p>"},{"location":"gallery.html","title":"Gallery","text":"<p>This page shows a gallery of the capabilities of the HPC MultiBench tool.</p>"},{"location":"gallery.html#viewing-run-configurations","title":"Viewing run configurations","text":"A screenshot of the TUI viewing a run configuration, including the instantiation of variables and the Slurm sbatch file to run."},{"location":"gallery.html#viewing-aggregated-metrics","title":"Viewing aggregated metrics","text":"A screenshot of the TUI viewing the metric results extracted from and aggregated across a set of runs defined in the YAML file."},{"location":"gallery.html#dispatching-runs","title":"Dispatching runs","text":"A screenshot of the TUI showing a modal dialog as it spawns Slurm jobs after  the \"Run Test Plan\" button is pressed."},{"location":"gallery.html#line-plots","title":"Line plots","text":"A screenshot of a a line plot of the metrics rendered entirely in the terminal. A screenshot of a a line plot of the metrics rendered entirely in the terminal. A screenshot of a Matplotlib window spawned by the TUI displaying a line plot including error bars of the metrics."},{"location":"gallery.html#bar-plots","title":"Bar plots","text":"A screenshot of a a bar plot of the metrics rendered entirely in the terminal. A screenshot of a Matplotlib window spawned by the TUI displaying a bar plot including error bars of the metrics."},{"location":"gallery.html#roofline-plots","title":"Roofline plots","text":"A screenshot of a a roofline plot of the metrics rendered entirely in the terminal. A screenshot of a Matplotlib window spawned by the TUI displaying a roofline plot including error bars of the metrics."},{"location":"manual.html","title":"Manual","text":"<p>When the <code>hpc-multibench</code> tool and its subcommands are invoked with the <code>-h</code> or <code>--help</code> flags, the following help pages are displayed.</p>"},{"location":"manual.html#top-level-help-page","title":"Top-level help page","text":"<pre><code>usage: __main__.py [-h] -y YAML_PATH [-o OUTPUTS_DIRECTORY] {record,interactive,report} ...\n\nA Swiss army knife for comparing programs on HPC resources.\n\npositional arguments:\n  {record,interactive,report}\n    record              record data from running the test benches\n    interactive         show the interactive TUI\n    report              report analysis about completed test bench runs\n\noptions:\n  -h, --help            show this help message and exit\n  -y YAML_PATH, --yaml-path YAML_PATH\n                        the path to the configuration YAML file\n  -o OUTPUTS_DIRECTORY, --outputs-directory OUTPUTS_DIRECTORY\n                        the path to the configuration YAML file\n</code></pre>"},{"location":"manual.html#record-subcommand-help-page","title":"<code>record</code> subcommand help page","text":"<pre><code>usage: __main__.py record [-h] [-d] [-w] [-nc]\n\noptions:\n  -h, --help         show this help message and exit\n  -d, --dry-run      print but don't submit the generated sbatch files\n  -w, --wait         wait for the submitted jobs to finish to exit\n  -nc, --no-clobber  don't delete any previous run results of the test benches\n</code></pre>"},{"location":"manual.html#report-subcommand-help-page","title":"<code>report</code> subcommand help page","text":"<pre><code>usage: __main__.py report [-h]\n\noptions:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"manual.html#interactive-subcommand-help-page","title":"<code>interactive</code> subcommand help page","text":"<pre><code>usage: __main__.py interactive [-h] [-nc]\n\noptions:\n  -h, --help         show this help message and exit\n  -nc, --no-clobber  don't delete any previous run results of the test benches\n</code></pre>"},{"location":"quickstart.html","title":"Quick start","text":"<p>The following sections describe how to use the HPC MultiBench tool from the command line.</p>"},{"location":"quickstart.html#installation","title":"Installation","text":"<p>To install the tool, clone and navigate to the repository, then use poetry to create a virtual environment as follows:</p> <pre><code>git clone --recurse-submodules -j8 https://github.com/EdmundGoodman/hpc-multibench\ncd hpc-multibench\npoetry install --without docs,test,dev\n</code></pre>"},{"location":"quickstart.html#interactively-reviewing-sample-results","title":"Interactively reviewing sample results","text":"<p>Using the <code>parallelism</code> test plan in the <code>hpccg-rs-kudu-results</code> submodule as an example, we can interactively view the data as follows:</p> <pre><code>poetry run hpc-multibench \\\n    -y generated_results/hpccg-rs-kudu-results/_test_plans/parallelism.yaml \\\n    -o generated_results/hpccg-rs-kudu-results/ \\\n    interactive\n</code></pre> <p>This will open a terminal-user interface allowing interactive visualisation of results. This is rendered inside the terminal, and as such does not require X-forwarding to be set up to present data and plot graphs.</p> <p>We can see the required <code>-y</code> flag being used to select the YAML file for the test plan, and the option <code>-o</code> flag to point to the directory containing the sample data. The <code>interactive</code> subcommand then runs the program in interactice mode.</p>"},{"location":"quickstart.html#dispatching-runs","title":"Dispatching runs","text":"<p>On a system with Slurm installed, runs can be dispatched as follows:</p> <pre><code>poetry run hpc-multibench \\\n    -y generated_results/hpccg-rs-kudu-results/_test_plans/parallelism.yaml \\\n    record\n</code></pre> <p>Since the <code>-o</code> flag is not specified here, it will default to writing out the files to a directory called <code>results/</code> at the root of the repository.</p>"},{"location":"quickstart.html#reviewing-runs-non-interactively","title":"Reviewing runs non-interactively","text":"<p>Run results can also be viewed non-interactively as follows:</p> <pre><code>poetry run hpc-multibench \\\n    -y generated_results/hpccg-rs-kudu-results/_test_plans/parallelism.yaml \\\n    -o generated_results/hpccg-rs-kudu-results/ \\\n    report\n</code></pre> <p>This will open a sequence of matplotlib windows and write out any export data as specified within the YAML file.</p>"},{"location":"yaml_schema.html","title":"YAML Schema","text":"<p>The YAML schema is the key abstraction which allows similar workflows for performance analysis to be captured by this single tool.</p> <p>Pydantic is used to parse the YAML file, and as such a full description of the YAML schema can be seen in the entity-relationship diagram of the Pydantic model, shown below.</p> A diagram of the Pydantic model used to parse the YAML schema, generated using erdantic."},{"location":"reference/main.html","title":"Reference","text":"A UML diagram showing the package relationships within the tool."},{"location":"reference/main.html#hpc_multibench.main","title":"<code>main</code>","text":"<p>The main function for the HPC MultiBench tool.</p>"},{"location":"reference/main.html#hpc_multibench.main.get_parser","title":"<code>get_parser() -&gt; ArgumentParser</code>","text":"<p>Get the argument parser for the tool.</p> Source code in <code>src/hpc_multibench/main.py</code> <pre><code>def get_parser() -&gt; ArgumentParser:  # pragma: no cover\n    \"\"\"Get the argument parser for the tool.\"\"\"\n    parser = ArgumentParser(\n        description=\"A Swiss army knife for comparing programs on HPC resources.\"\n    )\n    # As an argument of the base tool not subcommands, due to the ergonomics of\n    # running `record` then `report` on same yaml file in sequence\n    parser.add_argument(\n        \"-y\",\n        \"--yaml-path\",\n        required=True,\n        type=Path,\n        help=\"the path to the configuration YAML file\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--outputs-directory\",\n        type=Path,\n        default=DEFAULT_BASE_OUTPUTS_DIRECTORY,\n        help=\"the path to the configuration YAML file\",\n    )\n\n    sub_parsers = parser.add_subparsers(dest=\"command\", required=True)\n    parser_record = sub_parsers.add_parser(\n        \"record\", help=\"record data from running the test benches\"\n    )\n    parser_interactive = sub_parsers.add_parser(\n        \"interactive\", help=\"show the interactive TUI\"\n    )\n    sub_parsers.add_parser(\n        \"report\", help=\"report analysis about completed test bench runs\"\n    )\n\n    parser_record.add_argument(\n        \"-d\",\n        \"--dry-run\",\n        action=\"store_true\",\n        help=\"print but don't submit the generated sbatch files\",\n    )\n    parser_record.add_argument(\n        \"-w\",\n        \"--wait\",\n        action=\"store_true\",\n        help=\"wait for the submitted jobs to finish to exit\",\n    )\n    for sub_parser in (parser_record, parser_interactive):\n        sub_parser.add_argument(\n            \"-nc\",\n            \"--no-clobber\",\n            action=\"store_true\",\n            help=\"don't delete any previous run results of the test benches\",\n        )\n    return parser\n</code></pre>"},{"location":"reference/main.html#hpc_multibench.main.main","title":"<code>main() -&gt; None</code>","text":"<p>Run the tool.</p> Source code in <code>src/hpc_multibench/main.py</code> <pre><code>def main() -&gt; None:  # pragma: no cover\n    \"\"\"Run the tool.\"\"\"\n    args = get_parser().parse_args()\n    test_plan = TestPlan(args.yaml_path, args.outputs_directory)\n\n    if args.command == \"record\":\n        test_plan.record_all(args)\n\n    elif args.command == \"report\":\n        test_plan.report_all(args)\n\n    else:\n        args.dry_run = False\n        args.wait = False\n        UserInterface(test_plan, args).run()\n</code></pre>"},{"location":"reference/main.html#hpc_multibench.test_plan","title":"<code>test_plan</code>","text":"<p>A class representing the test plan defined from YAML for a tool run.</p>"},{"location":"reference/main.html#hpc_multibench.test_plan.TestPlan","title":"<code>TestPlan</code>","text":"<p>The test plan defined from YAML for a tool run.</p> Source code in <code>src/hpc_multibench/test_plan.py</code> <pre><code>class TestPlan:\n    \"\"\"The test plan defined from YAML for a tool run.\"\"\"\n\n    def __init__(self, yaml_path: Path, base_output_directory: Path) -&gt; None:\n        \"\"\"Instantiate the test plan from a YAML file.\"\"\"\n        self.yaml_path = yaml_path\n        self.base_output_directory = base_output_directory\n\n        test_plan_model = TestPlanModel.from_yaml(yaml_path)\n        self.benches = [\n            TestBench(\n                name=bench_name,\n                run_configuration_models={\n                    name: deepcopy(config)\n                    for name, config in test_plan_model.run_configurations.items()\n                    if name in bench_model.run_configurations\n                },\n                bench_model=bench_model,\n                base_output_directory=base_output_directory,\n            )\n            for bench_name, bench_model in test_plan_model.benches.items()\n        ]\n\n    def record_all(self, args: Namespace) -&gt; None:\n        \"\"\"Run all the enabled test benches in the plan.\"\"\"\n        for bench in self.benches:\n            if bench.bench_model.enabled:\n                print(f\"Recording data from test bench '{bench.name}'\")\n                bench.record(args)\n\n        if args.wait:\n            for bench in self.benches:\n                if bench.bench_model.enabled:\n                    status = (\n                        \"timed out while waiting for queued jobs\"\n                        if bench.wait_for_queue()\n                        else \"finished all queued jobs\"\n                    )\n                    print(f\"Test bench '{bench.name}' {status}!\")\n\n    def report_all(self, _args: Namespace) -&gt; None:\n        \"\"\"Analyse all the enabled test benches in the plan.\"\"\"\n        for bench in self.benches:\n            if bench.bench_model.enabled:\n                print(f\"Reporting data from test bench '{bench.name}'\")\n                bench.report()\n</code></pre>"},{"location":"reference/main.html#hpc_multibench.test_plan.TestPlan.__init__","title":"<code>__init__(yaml_path: Path, base_output_directory: Path) -&gt; None</code>","text":"<p>Instantiate the test plan from a YAML file.</p> Source code in <code>src/hpc_multibench/test_plan.py</code> <pre><code>def __init__(self, yaml_path: Path, base_output_directory: Path) -&gt; None:\n    \"\"\"Instantiate the test plan from a YAML file.\"\"\"\n    self.yaml_path = yaml_path\n    self.base_output_directory = base_output_directory\n\n    test_plan_model = TestPlanModel.from_yaml(yaml_path)\n    self.benches = [\n        TestBench(\n            name=bench_name,\n            run_configuration_models={\n                name: deepcopy(config)\n                for name, config in test_plan_model.run_configurations.items()\n                if name in bench_model.run_configurations\n            },\n            bench_model=bench_model,\n            base_output_directory=base_output_directory,\n        )\n        for bench_name, bench_model in test_plan_model.benches.items()\n    ]\n</code></pre>"},{"location":"reference/main.html#hpc_multibench.test_plan.TestPlan.record_all","title":"<code>record_all(args: Namespace) -&gt; None</code>","text":"<p>Run all the enabled test benches in the plan.</p> Source code in <code>src/hpc_multibench/test_plan.py</code> <pre><code>def record_all(self, args: Namespace) -&gt; None:\n    \"\"\"Run all the enabled test benches in the plan.\"\"\"\n    for bench in self.benches:\n        if bench.bench_model.enabled:\n            print(f\"Recording data from test bench '{bench.name}'\")\n            bench.record(args)\n\n    if args.wait:\n        for bench in self.benches:\n            if bench.bench_model.enabled:\n                status = (\n                    \"timed out while waiting for queued jobs\"\n                    if bench.wait_for_queue()\n                    else \"finished all queued jobs\"\n                )\n                print(f\"Test bench '{bench.name}' {status}!\")\n</code></pre>"},{"location":"reference/main.html#hpc_multibench.test_plan.TestPlan.report_all","title":"<code>report_all(_args: Namespace) -&gt; None</code>","text":"<p>Analyse all the enabled test benches in the plan.</p> Source code in <code>src/hpc_multibench/test_plan.py</code> <pre><code>def report_all(self, _args: Namespace) -&gt; None:\n    \"\"\"Analyse all the enabled test benches in the plan.\"\"\"\n    for bench in self.benches:\n        if bench.bench_model.enabled:\n            print(f\"Reporting data from test bench '{bench.name}'\")\n            bench.report()\n</code></pre>"},{"location":"reference/plot.html","title":"Reference","text":""},{"location":"reference/plot.html#hpc_multibench.plot.plot_data","title":"<code>plot_data</code>","text":"<p>A set of functions to get the data series to plot for test run results.</p>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_data.split_metric_uncertainty","title":"<code>split_metric_uncertainty(metrics: dict[str, str | UFloat], metric: str) -&gt; tuple[float, float | None]</code>","text":"<p>Get the uncertainty and value from a possible uncertain metric.</p> Source code in <code>src/hpc_multibench/plot/plot_data.py</code> <pre><code>def split_metric_uncertainty(\n    metrics: dict[str, str | UFloat], metric: str\n) -&gt; tuple[float, float | None]:\n    \"\"\"Get the uncertainty and value from a possible uncertain metric.\"\"\"\n    value = metrics[metric]\n    if isinstance(value, UFloat):\n        return (value.nominal_value, value.std_dev)\n    return (float(value), None)\n</code></pre>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_data.get_line_plot_data","title":"<code>get_line_plot_data(plot: LinePlotModel, all_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]) -&gt; dict[str, tuple[list[float], list[float], list[float] | None, list[float] | None]]</code>","text":"<p>Get the data needed to plot a specified line plot for a set of runs.</p> Source code in <code>src/hpc_multibench/plot/plot_data.py</code> <pre><code>def get_line_plot_data(\n    plot: LinePlotModel,\n    all_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]],\n) -&gt; dict[str, tuple[list[float], list[float], list[float] | None, list[float] | None]]:\n    \"\"\"Get the data needed to plot a specified line plot for a set of runs.\"\"\"\n    # Reshape the metrics data from multiple runs into groups of points\n    data: dict[str, list[tuple[float, float, float | None, float | None]]] = {}\n    for run_configuration, metrics in all_metrics:\n        split_names: list[str] = [\n            f\"{split_metric}={metrics[split_metric]}\"\n            for split_metric in plot.split_metrics\n            if split_metric not in plot.fix_metrics\n        ]\n        fix_names: list[str] = [\n            f\"{metric}={value}\" for metric, value in plot.fix_metrics.items()\n        ]\n        series_identifier = \", \".join(\n            [run_configuration.name, *fix_names, *split_names]\n        )\n\n        if any(\n            metrics[metric] != str(value) for metric, value in plot.fix_metrics.items()\n        ):\n            continue\n\n        (x_value, x_err) = split_metric_uncertainty(metrics, plot.x)\n\n        y_metrics = plot.y if isinstance(plot.y, list) else [plot.y]\n        for y_metric in y_metrics:\n            series_name = (\n                f\"{y_metric}, {series_identifier}\"\n                if isinstance(plot.y, list)\n                else series_identifier\n            )\n            (y_value, y_err) = split_metric_uncertainty(metrics, y_metric)\n            if series_name not in data:\n                data[series_name] = []\n            data[series_name].append((x_value, y_value, x_err, y_err))\n\n    # Further reshape the data into convenient data series\n    reshaped_data: dict[\n        str, tuple[list[float], list[float], list[float] | None, list[float] | None]\n    ] = {}\n    for name, results in data.items():\n        x, y, x_err, y_err = cast(  # type: ignore[assignment]\n            tuple[list[float], list[float], list[float | None], list[float | None]],\n            zip(*sorted(results), strict=True),\n        )\n        reshaped_data[name] = (  # type: ignore[assignment]\n            x,\n            y,\n            x_err if any(x_err) else None,  # type: ignore[arg-type]\n            y_err if any(y_err) else None,  # type: ignore[arg-type]\n        )\n    return reshaped_data\n</code></pre>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_data.get_bar_chart_data","title":"<code>get_bar_chart_data(plot: BarChartModel, all_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]) -&gt; dict[str, tuple[float, float | None, int]]</code>","text":"<p>Get the data needed to plot a specified bar chart for a set of runs.</p> Source code in <code>src/hpc_multibench/plot/plot_data.py</code> <pre><code>def get_bar_chart_data(\n    plot: BarChartModel,\n    all_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]],\n) -&gt; dict[str, tuple[float, float | None, int]]:\n    \"\"\"Get the data needed to plot a specified bar chart for a set of runs.\"\"\"\n    data: dict[str, tuple[float, float | None, int]] = {}\n\n    # Extract the outputs into the data format needed for the line plot\n    hue_index_lookup: dict[str, int] = {}\n    new_hue_index = 0\n    for run_configuration, metrics in all_metrics:\n        split_names: list[str] = [\n            f\"{split_metric}={metrics[split_metric]}\"\n            for split_metric in plot.split_metrics\n            if split_metric not in plot.fix_metrics\n        ]\n        fix_names: list[str] = [\n            f\"{metric}={value}\" for metric, value in plot.fix_metrics.items()\n        ]\n        series_identifier = \", \".join(\n            [run_configuration.name, *fix_names, *split_names]\n        )\n\n        if any(\n            metrics[metric] != str(value) for metric, value in plot.fix_metrics.items()\n        ):\n            continue\n\n        if run_configuration.name not in hue_index_lookup:\n            hue_index_lookup[run_configuration.name] = new_hue_index\n            new_hue_index += 1\n\n        y_metrics = plot.y if isinstance(plot.y, list) else [plot.y]\n        for y_metric in y_metrics:\n            series_name = (\n                f\"{y_metric}, {series_identifier}\"\n                if isinstance(plot.y, list)\n                else series_identifier\n            )\n            (y_value, y_err) = split_metric_uncertainty(metrics, y_metric)\n            data[series_name] = (\n                y_value,\n                y_err,\n                hue_index_lookup[run_configuration.name],\n            )\n    return data\n</code></pre>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_data.get_roofline_plot_data","title":"<code>get_roofline_plot_data(plot: RooflinePlotModel, all_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]) -&gt; tuple[RooflineDataModel, dict[str, tuple[float, float, float | None, float | None]]]</code>","text":"<p>Get the data needed to plot a specified roofline plot.</p> Source code in <code>src/hpc_multibench/plot/plot_data.py</code> <pre><code>def get_roofline_plot_data(\n    plot: RooflinePlotModel,\n    all_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]],\n) -&gt; tuple[\n    RooflineDataModel, dict[str, tuple[float, float, float | None, float | None]]\n]:\n    \"\"\"Get the data needed to plot a specified roofline plot.\"\"\"\n    roofline_data = RooflineDataModel.from_json(plot.ert_json)\n\n    data: dict[str, tuple[float, float, float | None, float | None]] = {}\n    for run_configuration, metrics in all_metrics:\n        (y_value, y_err) = split_metric_uncertainty(metrics, plot.gflops_per_sec)\n        (x_value_tmp, x_err_tmp) = split_metric_uncertainty(\n            metrics, plot.mbytes_per_sec\n        )\n        x_value = y_value / (x_value_tmp / 1000)\n        x_err = None if x_err_tmp is None or y_err is None else y_err + x_err_tmp\n        data[run_configuration.name] = (x_value, y_value, x_err, y_err)\n\n    return (roofline_data, data)\n</code></pre>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_matplotlib","title":"<code>plot_matplotlib</code>","text":"<p>A set of functions using matplotlib to plot the results of a test bench run.</p>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_matplotlib.draw_line_plot","title":"<code>draw_line_plot(plot: LinePlotModel, metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]) -&gt; None</code>","text":"<p>Draw a specified line plot for a set of run outputs.</p> Source code in <code>src/hpc_multibench/plot/plot_matplotlib.py</code> <pre><code>def draw_line_plot(\n    plot: LinePlotModel,\n    metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]],\n) -&gt; None:\n    \"\"\"Draw a specified line plot for a set of run outputs.\"\"\"\n    data = get_line_plot_data(plot, metrics)\n\n    for name, (x, y, x_err, y_err) in data.items():\n        plt.errorbar(\n            x,\n            y,\n            xerr=x_err,\n            yerr=y_err,\n            marker=\"x\",\n            ecolor=\"black\",\n            label=name,\n        )\n    plt.legend()\n    plt.xlabel(plot.x)\n    if isinstance(plot.y, list):\n        plt.ylabel(\" | \".join(plot.y))\n    else:\n        plt.ylabel(plot.y)\n    plt.title(plot.title)\n    if plot.x_lim is not None:\n        plt.xlim(plot.x_lim)\n    if plot.y_lim is not None:\n        plt.ylim(plot.y_lim)\n    if plot.x_log:\n        plt.xscale(\"log\")\n    if plot.y_log:\n        plt.yscale(\"log\")\n    plt.show()\n</code></pre>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_matplotlib.draw_bar_chart","title":"<code>draw_bar_chart(plot: BarChartModel, metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]) -&gt; None</code>","text":"<p>Draw a specified bar chart for a set of run outputs.</p> Source code in <code>src/hpc_multibench/plot/plot_matplotlib.py</code> <pre><code>def draw_bar_chart(\n    plot: BarChartModel,\n    metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]],\n) -&gt; None:\n    \"\"\"Draw a specified bar chart for a set of run outputs.\"\"\"\n    data = get_bar_chart_data(plot, metrics)\n\n    # For matplotlib `plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]`\n    palette = sns.color_palette()\n    plt.barh(\n        list(data.keys()),\n        [metric for metric, _, _ in data.values()],\n        xerr=[uncertainty for _, uncertainty, _ in data.values()],\n        color=[palette[hue] for _, _, hue in data.values()],\n        ecolor=\"black\",\n    )\n    plt.xlabel(plot.y)\n    plt.gcf().subplots_adjust(left=0.25)\n    plt.title(plot.title)\n    if plot.y_lim is not None:\n        plt.ylim(plot.y_lim)\n    if plot.y_log:\n        plt.yscale(\"log\")\n    plt.show()\n</code></pre>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_matplotlib.draw_roofline_plot","title":"<code>draw_roofline_plot(plot: RooflinePlotModel, metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]) -&gt; None</code>","text":"<p>Draw a specified roofline plots for a set of run outputs.</p> Source code in <code>src/hpc_multibench/plot/plot_matplotlib.py</code> <pre><code>def draw_roofline_plot(\n    plot: RooflinePlotModel,\n    metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]],\n) -&gt; None:\n    \"\"\"Draw a specified roofline plots for a set of run outputs.\"\"\"\n    (roofline, data) = get_roofline_plot_data(plot, metrics)\n\n    for label, (x, y) in roofline.memory_bound_ceilings.items():\n        plt.plot(x, y, label=label)\n    for label, (x, y) in roofline.compute_bound_ceilings.items():\n        plt.plot(x, y, label=label)\n    # from labellines import labelLines\n    # for ax in plt.gcf().axes:\n    #     labelLines(ax.get_lines())\n    for name, (x_point, y_point, x_err, y_err) in data.items():\n        plt.errorbar(\n            x_point,\n            y_point,\n            xerr=x_err,\n            yerr=y_err,\n            marker=\"o\",\n            ecolor=\"black\",\n            label=name,\n        )\n    plt.legend()\n    plt.xlabel(\"FLOPs/Byte\")\n    plt.ylabel(\"GFLOPs/sec\")\n    plt.xscale(\"log\")\n    plt.yscale(\"log\")\n    plt.title(plot.title)\n    plt.show()\n</code></pre>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_plotext","title":"<code>plot_plotext</code>","text":"<p>A set of functions using plotext to plot the results of a test bench run.</p>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_plotext.draw_line_plot","title":"<code>draw_line_plot(this_plt: Any, plot: LinePlotModel, metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]) -&gt; None</code>","text":"<p>Draw a line plot from data using the provided plotext backend.</p> Source code in <code>src/hpc_multibench/plot/plot_plotext.py</code> <pre><code>def draw_line_plot(\n    this_plt: Any,\n    plot: LinePlotModel,\n    metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]],\n) -&gt; None:\n    \"\"\"Draw a line plot from data using the provided plotext backend.\"\"\"\n    data = get_line_plot_data(plot, metrics)\n\n    this_plt.clear_figure()\n    for name, (x, y, _x_err, _y_err) in data.items():\n        this_plt.plot(x, y, marker=PLOTEXT_MARKER, label=name)\n    this_plt.theme(PLOTEXT_THEME)\n    this_plt.xlabel(plot.x)\n    if isinstance(plot.y, list):\n        this_plt.ylabel(\" | \".join(plot.y))\n    else:\n        this_plt.ylabel(plot.y)\n    this_plt.ylim(0)\n    if plot.x_log:\n        this_plt.xscale(\"log\")\n    if plot.y_log:\n        this_plt.yscale(\"log\")\n    this_plt.title(plot.title)\n</code></pre>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_plotext.draw_bar_chart","title":"<code>draw_bar_chart(this_plt: Any, plot: BarChartModel, metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]) -&gt; None</code>","text":"<p>Draw a line plot from data using the provided plotext backend.</p> Source code in <code>src/hpc_multibench/plot/plot_plotext.py</code> <pre><code>def draw_bar_chart(\n    this_plt: Any,\n    plot: BarChartModel,\n    metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]],\n) -&gt; None:\n    \"\"\"Draw a line plot from data using the provided plotext backend.\"\"\"\n    data = get_bar_chart_data(plot, metrics)\n\n    this_plt.clear_figure()\n    this_plt.bar(\n        data.keys(),\n        [metric for metric, _, _ in data.values()],\n        orientation=\"horizontal\",\n        width=3 / 5,\n    )\n    this_plt.theme(PLOTEXT_THEME)\n    this_plt.ylabel(plot.y)\n    this_plt.title(plot.title)\n    if plot.y_log:\n        this_plt.yscale(\"log\")\n</code></pre>"},{"location":"reference/plot.html#hpc_multibench.plot.plot_plotext.draw_roofline_plot","title":"<code>draw_roofline_plot(this_plt: Any, plot: RooflinePlotModel, metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]) -&gt; None</code>","text":"<p>Draw a roofline plot from data using the provided plotext backend.</p> Source code in <code>src/hpc_multibench/plot/plot_plotext.py</code> <pre><code>def draw_roofline_plot(\n    this_plt: Any,\n    plot: RooflinePlotModel,\n    metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]],\n) -&gt; None:\n    \"\"\"Draw a roofline plot from data using the provided plotext backend.\"\"\"\n    (roofline, data) = get_roofline_plot_data(plot, metrics)\n\n    this_plt.clear_figure()\n    for label, (x, y) in roofline.memory_bound_ceilings.items():\n        this_plt.plot(x, y, label=label, marker=PLOTEXT_MARKER)\n    for label, (x, y) in roofline.compute_bound_ceilings.items():\n        this_plt.plot(x, y, label=label, marker=PLOTEXT_MARKER)\n    for name, (x_point, y_point, x_err, y_err) in data.items():\n        this_plt.error(\n            [x_point],\n            [y_point],\n            xerr=[x_err / 2] * 2 if x_err is not None else None,\n            yerr=[y_err / 2] * 2 if y_err is not None else None,\n            label=name,\n        )\n    this_plt.theme(PLOTEXT_THEME)\n    this_plt.xlabel(\"FLOPs/Byte\")\n    this_plt.ylabel(\"GFLOPs/sec\")\n    this_plt.xscale(\"log\")\n    this_plt.yscale(\"log\")\n    this_plt.title(plot.title)\n</code></pre>"},{"location":"reference/plot.html#hpc_multibench.plot.export_data","title":"<code>export_data</code>","text":"<p>A set of functions to export the results of a test bench run.</p>"},{"location":"reference/plot.html#hpc_multibench.plot.export_data.export_data","title":"<code>export_data(plot: ExportModel, all_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]) -&gt; None</code>","text":"<p>Construct and export a pandas data frame from the metrics.</p> Source code in <code>src/hpc_multibench/plot/export_data.py</code> <pre><code>def export_data(\n    plot: ExportModel,\n    all_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]],\n) -&gt; None:\n    \"\"\"Construct and export a pandas data frame from the metrics.\"\"\"\n    df_data: dict[str, list[float | str]] = {}\n    for run_configuration, metrics in all_metrics:\n        row_data: dict[str, float | str] = {\"Run configuration\": run_configuration.name}\n\n        for metric in metrics:\n            value, error = split_metric_uncertainty(metrics, metric)\n            row_data[metric] = value\n            if error is not None:\n                row_data[f\"{metric} error\"] = error\n\n        for column, cell in row_data.items():\n            if column not in df_data:\n                df_data[column] = []\n            df_data[column].append(cell)\n\n    export_df = pd.DataFrame(df_data)\n\n    if plot.export_path is None:\n        print(export_df.to_string())\n    elif plot.export_format == \"csv\":\n        export_df.to_csv(plot.export_path)\n    elif plot.export_format == \"latex\":\n        export_df.to_latex(plot.export_path)\n    else:\n        raise NotImplementedError(\n            f\"Export format '{plot.export_format}' not supported!\"\n        )\n</code></pre>"},{"location":"reference/roofline_model.html","title":"Reference","text":""},{"location":"reference/roofline_model.html#hpc_multibench.roofline_model","title":"<code>roofline_model</code>","text":"<p>A set of objects modelling the schema for the ERT roofline JSON file.</p>"},{"location":"reference/roofline_model.html#hpc_multibench.roofline_model.MetricsModel","title":"<code>MetricsModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The data schema for metrics in the ERT JSON schema.</p> Source code in <code>src/hpc_multibench/roofline_model.py</code> <pre><code>class MetricsModel(BaseModel):\n    \"\"\"The data schema for metrics in the ERT JSON schema.\"\"\"\n\n    data: list[tuple[str, float]]\n    metadata: dict[str, Any]\n</code></pre>"},{"location":"reference/roofline_model.html#hpc_multibench.roofline_model.EmpiricalModel","title":"<code>EmpiricalModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The data schema for the empirical key in the ERT JSON schema.</p> Source code in <code>src/hpc_multibench/roofline_model.py</code> <pre><code>class EmpiricalModel(BaseModel):\n    \"\"\"The data schema for the empirical key in the ERT JSON schema.\"\"\"\n\n    metadata: dict[str, Any]\n    gflops: MetricsModel\n    gbytes: MetricsModel\n</code></pre>"},{"location":"reference/roofline_model.html#hpc_multibench.roofline_model.ErtJsonModel","title":"<code>ErtJsonModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The data schema for the ERT JSON schema.</p> Source code in <code>src/hpc_multibench/roofline_model.py</code> <pre><code>class ErtJsonModel(BaseModel):\n    \"\"\"The data schema for the ERT JSON schema.\"\"\"\n\n    model_config = ConfigDict(strict=True)\n\n    empirical: EmpiricalModel\n    spec: dict[str, Any]\n</code></pre>"},{"location":"reference/roofline_model.html#hpc_multibench.roofline_model.RooflineDataModel","title":"<code>RooflineDataModel</code>  <code>dataclass</code>","text":"<p>The extracted relevant data from the ERT JSON schema.</p> Source code in <code>src/hpc_multibench/roofline_model.py</code> <pre><code>@dataclass\nclass RooflineDataModel:\n    \"\"\"The extracted relevant data from the ERT JSON schema.\"\"\"\n\n    gflops_per_sec: dict[str, float]\n    gbytes_per_sec: dict[str, float]\n\n    @classmethod\n    def from_json(cls, ert_json: Path) -&gt; Self:\n        \"\"\"Extract the relevant roofline data from an ERT JSON file.\"\"\"\n        json_data = ert_json.read_text(\"utf-8\")\n        parsed_data = ErtJsonModel.model_validate_json(json_data)\n        return cls(\n            gflops_per_sec=dict(parsed_data.empirical.gflops.data),\n            gbytes_per_sec=dict(parsed_data.empirical.gbytes.data),\n        )\n\n    @property\n    def memory_bound_ceilings(self) -&gt; dict[str, tuple[list[float], list[float]]]:\n        \"\"\"Get a labelled set of memory bound ceiling lines.\"\"\"\n        memory_bound_ceilings: dict[str, tuple[list[float], list[float]]] = {}\n        for ceiling_name, m in self.gbytes_per_sec.items():\n            y_values = [1, *list(self.gflops_per_sec.values())]\n            x_values = [y / m for y in y_values]\n            ceiling_label = f\"{ceiling_name} = {m} GB/s\"\n            memory_bound_ceilings[ceiling_label] = (x_values, y_values)\n        return memory_bound_ceilings\n\n    @property\n    def compute_bound_ceilings(self) -&gt; dict[str, tuple[list[float], list[float]]]:\n        \"\"\"Get a labelled set of computer bound ceiling lines.\"\"\"\n        compute_bound_ceilings: dict[str, tuple[list[float], list[float]]] = {}\n        for ceiling_name, y in self.gflops_per_sec.items():\n            x_values = [\n                y / max(self.gbytes_per_sec.values()),\n                20 * (y / min(self.gbytes_per_sec.values())),\n            ]\n            y_values = [y, y]\n            ceiling_label = f\"{y} {ceiling_name}\"\n            compute_bound_ceilings[ceiling_label] = (x_values, y_values)\n        return compute_bound_ceilings\n</code></pre>"},{"location":"reference/roofline_model.html#hpc_multibench.roofline_model.RooflineDataModel.memory_bound_ceilings","title":"<code>memory_bound_ceilings: dict[str, tuple[list[float], list[float]]]</code>  <code>property</code>","text":"<p>Get a labelled set of memory bound ceiling lines.</p>"},{"location":"reference/roofline_model.html#hpc_multibench.roofline_model.RooflineDataModel.compute_bound_ceilings","title":"<code>compute_bound_ceilings: dict[str, tuple[list[float], list[float]]]</code>  <code>property</code>","text":"<p>Get a labelled set of computer bound ceiling lines.</p>"},{"location":"reference/roofline_model.html#hpc_multibench.roofline_model.RooflineDataModel.from_json","title":"<code>from_json(ert_json: Path) -&gt; Self</code>  <code>classmethod</code>","text":"<p>Extract the relevant roofline data from an ERT JSON file.</p> Source code in <code>src/hpc_multibench/roofline_model.py</code> <pre><code>@classmethod\ndef from_json(cls, ert_json: Path) -&gt; Self:\n    \"\"\"Extract the relevant roofline data from an ERT JSON file.\"\"\"\n    json_data = ert_json.read_text(\"utf-8\")\n    parsed_data = ErtJsonModel.model_validate_json(json_data)\n    return cls(\n        gflops_per_sec=dict(parsed_data.empirical.gflops.data),\n        gbytes_per_sec=dict(parsed_data.empirical.gbytes.data),\n    )\n</code></pre>"},{"location":"reference/run_configuration.html","title":"Reference","text":""},{"location":"reference/run_configuration.html#hpc_multibench.run_configuration","title":"<code>run_configuration</code>","text":"<p>A class for test configurations on batch compute.</p>"},{"location":"reference/run_configuration.html#hpc_multibench.run_configuration.RunConfiguration","title":"<code>RunConfiguration</code>","text":"<p>A builder class for a test run on batch compute.</p> Source code in <code>src/hpc_multibench/run_configuration.py</code> <pre><code>class RunConfiguration:\n    \"\"\"A builder class for a test run on batch compute.\"\"\"\n\n    def __init__(\n        self, name: str, run_command: str, output_directory: Path = Path(\"./\")\n    ):\n        \"\"\"Initialise the run configuration file as a empty bash file.\"\"\"\n        self.name = name\n        self.output_directory: Path = output_directory\n        self.sbatch_config: dict[str, str] = {}\n        self.module_loads: list[str] = []\n        self.environment_variables: dict[str, str] = {}\n        self.directory: Path | None = None\n        self.build_commands: list[str] = []\n        self.pre_built: bool = False\n        self.run_command: str = run_command\n        self.post_commands: list[str] = []\n        self.args: str | None = None\n        self.instantiation: dict[str, Any] | None = None\n\n    @property\n    def sbatch_contents(self) -&gt; str:  # noqa: C901\n        \"\"\"Construct the sbatch configuration for the run.\"\"\"\n        sbatch_file = SHELL_SHEBANG\n\n        for key, value in self.sbatch_config.items():\n            if key == \"output\":\n                # TODO: The output file will always override this key!\n                # This should probably be a logging statement...\n                print(\"WARNING: Output file configuration overriden!\")\n                continue\n            sbatch_file += f\"#SBATCH --{key}={value}\\n\"\n        sbatch_file += f\"#SBATCH --output={self.output_file}\\n\"\n\n        sbatch_file += \"\\necho '===== CONFIGURATION ====='\\n\"\n        if len(self.module_loads) &gt; 0:\n            sbatch_file += \"echo '=== MODULE LOADS ==='\\n\"\n            sbatch_file += \"module purge\\n\"\n            sbatch_file += f\"module load {' '.join(self.module_loads)}\\n\"\n\n        if len(self.environment_variables) &gt; 0:\n            sbatch_file += \"echo '=== ENVIRONMENT VARIABLES ==='\\n\"\n        for key, value in self.environment_variables.items():\n            sbatch_file += f\"export {key}={value}\\n\"\n            sbatch_file += f\"echo '{key}={value}'\\n\"\n\n        sbatch_file += \"echo '=== CPU ARCHITECTURE ==='\\n\"\n        sbatch_file += \"lscpu\\n\"\n        sbatch_file += \"echo '=== SLURM CONFIG ==='\\n\"\n        sbatch_file += \"scontrol show job $SLURM_JOB_ID\\n\"\n        if self.instantiation is not None:\n            sbatch_file += \"echo '=== RUN INSTANTIATION ==='\\n\"\n            sbatch_file += f\"echo '{self.instantiation}'\\n\"\n        sbatch_file += \"echo\\n\"\n\n        sbatch_file += \"\\necho '===== BUILD ====='\\n\"\n        if self.directory is not None:\n            sbatch_file += f\"cd {self.directory}\\n\"\n        if self.pre_built:\n            sbatch_file += \"echo 'run configuration was pre-built'\\n\"\n        else:\n            sbatch_file += \"\\n\".join(self.build_commands) + \"\\n\"\n        sbatch_file += \"echo\\n\"\n\n        sbatch_file += \"\\necho '===== RUN ====='\\n\"\n        if self.args is None:\n            sbatch_file += f\"{TIME_COMMAND}{self.run_command}\\n\"\n        else:\n            sbatch_file += f\"{TIME_COMMAND}{self.run_command} {self.args}\\n\"\n\n        if len(self.post_commands) &gt; 0:\n            sbatch_file += \"\\necho '===== POST RUN ====='\\n\"\n            sbatch_file += \"\\n\".join(self.post_commands)\n\n        return sbatch_file\n\n    @property\n    def output_file(self) -&gt; Path:\n        \"\"\"Get the path to the output file to write to.\"\"\"\n        # instantation_str = (\n        #     f\"__{RunConfiguration.get_instantiation_repr(self.instantiation)}\"\n        #     if self.instantiation is not None\n        #     else \"\"\n        # )\n        return self.output_directory / f\"{self.name}__%j.out\"\n\n    @classmethod\n    def get_instantiation_repr(cls, instantiation: dict[str, Any]) -&gt; str:\n        \"\"\"Get a string representation of a run instantiation.\"\"\"\n        # TODO: Better representation of sbatch etc than stringifying\n        # instantiation_items: list[str] = []\n        # for name, value in instantiation.items():\n        #     if name == \"args\":\n        #         instantiation_items.append(\n        #             f\"{name}={str(value).replace('/','').replace(' ','_')}\"\n        #         )\n        #     # elif name == \"run_command\":\n        #     # elif name == \"build_commands\":\n        #     # elif name == \"module_loads\":\n        #     # elif name == \"sbatch_config\":\n        #     # elif name == \"environment_variables\":\n        # return \",\".join(instantiation_items)\n        return \",\".join(\n            f\"{name}={str(value).replace('/','').replace(' ','_')}\"\n            for name, value in instantiation.items()\n        )\n\n    def get_true_output_file_name(self, slurm_id: int) -&gt; str:\n        \"\"\"Get the actual output file name with substituted slurm job id.\"\"\"\n        return f\"{self.output_file.name[:-8]}__{slurm_id}.out\"\n\n    def run(self, dependencies: list[int] | None = None) -&gt; int | None:\n        \"\"\"\n        Run the specified run configuration.\n\n        TODO: Could type alias for slurm job id for eturn type?\n        \"\"\"\n        # Ensure the output directory exists before it is used\n        self.output_file.parent.mkdir(parents=True, exist_ok=True)\n\n        # Create and run the temporary sbatch file via slurm\n        with NamedTemporaryFile(\n            prefix=self.name, suffix=\".sbatch\", dir=Path(\"./\"), mode=\"w+\"\n        ) as sbatch_tmp:\n            sbatch_tmp.write(self.sbatch_contents)\n            sbatch_tmp.flush()\n            command_list = [\"sbatch\", Path(sbatch_tmp.name)]\n            if dependencies is not None:\n                dependencies_string = \",\".join(str(job_id) for job_id in dependencies)\n                command_list.insert(1, f\"--dependency=afterok:{dependencies_string}\")\n            try:\n                result = subprocess_run(  # nosec\n                    command_list,  # type: ignore # noqa: S603, PGH003\n                    check=True,\n                    stdout=PIPE,\n                )\n            except CalledProcessError:\n                return None\n            job_id_search = re_search(SLURM_JOB_ID_REGEX, result.stdout.decode(\"utf-8\"))\n            if job_id_search is None:\n                return None\n            return int(job_id_search.group(1))\n\n    def collect(\n        self, slurm_id: int, check_queue: bool = False  # noqa: FBT001, FBT002\n    ) -&gt; str | None:\n        \"\"\"Collect the output from a completed job with a given slurm id.\"\"\"\n        # Check the job is completed in the queue\n        if check_queue:\n            result = subprocess_run(  # nosec\n                [\"squeue\", \"-j\", str(slurm_id)],  # noqa: S603, S607\n                check=True,\n                stdout=PIPE,\n            )\n            if SLURM_UNQUEUED_SUBSTRING in result.stdout.decode(\"utf-8\"):\n                return None\n\n        # Return the contents of the specified output file\n        output_file = self.output_file.parent / self.get_true_output_file_name(slurm_id)\n        if not output_file.exists():\n            return None\n        return output_file.read_text(encoding=\"utf-8\")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the sbatch configuration file defining the run.\"\"\"\n        return self.sbatch_contents\n</code></pre>"},{"location":"reference/run_configuration.html#hpc_multibench.run_configuration.RunConfiguration.sbatch_contents","title":"<code>sbatch_contents: str</code>  <code>property</code>","text":"<p>Construct the sbatch configuration for the run.</p>"},{"location":"reference/run_configuration.html#hpc_multibench.run_configuration.RunConfiguration.output_file","title":"<code>output_file: Path</code>  <code>property</code>","text":"<p>Get the path to the output file to write to.</p>"},{"location":"reference/run_configuration.html#hpc_multibench.run_configuration.RunConfiguration.__init__","title":"<code>__init__(name: str, run_command: str, output_directory: Path = Path('./'))</code>","text":"<p>Initialise the run configuration file as a empty bash file.</p> Source code in <code>src/hpc_multibench/run_configuration.py</code> <pre><code>def __init__(\n    self, name: str, run_command: str, output_directory: Path = Path(\"./\")\n):\n    \"\"\"Initialise the run configuration file as a empty bash file.\"\"\"\n    self.name = name\n    self.output_directory: Path = output_directory\n    self.sbatch_config: dict[str, str] = {}\n    self.module_loads: list[str] = []\n    self.environment_variables: dict[str, str] = {}\n    self.directory: Path | None = None\n    self.build_commands: list[str] = []\n    self.pre_built: bool = False\n    self.run_command: str = run_command\n    self.post_commands: list[str] = []\n    self.args: str | None = None\n    self.instantiation: dict[str, Any] | None = None\n</code></pre>"},{"location":"reference/run_configuration.html#hpc_multibench.run_configuration.RunConfiguration.get_instantiation_repr","title":"<code>get_instantiation_repr(instantiation: dict[str, Any]) -&gt; str</code>  <code>classmethod</code>","text":"<p>Get a string representation of a run instantiation.</p> Source code in <code>src/hpc_multibench/run_configuration.py</code> <pre><code>@classmethod\ndef get_instantiation_repr(cls, instantiation: dict[str, Any]) -&gt; str:\n    \"\"\"Get a string representation of a run instantiation.\"\"\"\n    # TODO: Better representation of sbatch etc than stringifying\n    # instantiation_items: list[str] = []\n    # for name, value in instantiation.items():\n    #     if name == \"args\":\n    #         instantiation_items.append(\n    #             f\"{name}={str(value).replace('/','').replace(' ','_')}\"\n    #         )\n    #     # elif name == \"run_command\":\n    #     # elif name == \"build_commands\":\n    #     # elif name == \"module_loads\":\n    #     # elif name == \"sbatch_config\":\n    #     # elif name == \"environment_variables\":\n    # return \",\".join(instantiation_items)\n    return \",\".join(\n        f\"{name}={str(value).replace('/','').replace(' ','_')}\"\n        for name, value in instantiation.items()\n    )\n</code></pre>"},{"location":"reference/run_configuration.html#hpc_multibench.run_configuration.RunConfiguration.get_true_output_file_name","title":"<code>get_true_output_file_name(slurm_id: int) -&gt; str</code>","text":"<p>Get the actual output file name with substituted slurm job id.</p> Source code in <code>src/hpc_multibench/run_configuration.py</code> <pre><code>def get_true_output_file_name(self, slurm_id: int) -&gt; str:\n    \"\"\"Get the actual output file name with substituted slurm job id.\"\"\"\n    return f\"{self.output_file.name[:-8]}__{slurm_id}.out\"\n</code></pre>"},{"location":"reference/run_configuration.html#hpc_multibench.run_configuration.RunConfiguration.run","title":"<code>run(dependencies: list[int] | None = None) -&gt; int | None</code>","text":"<p>Run the specified run configuration.</p> <p>TODO: Could type alias for slurm job id for eturn type?</p> Source code in <code>src/hpc_multibench/run_configuration.py</code> <pre><code>def run(self, dependencies: list[int] | None = None) -&gt; int | None:\n    \"\"\"\n    Run the specified run configuration.\n\n    TODO: Could type alias for slurm job id for eturn type?\n    \"\"\"\n    # Ensure the output directory exists before it is used\n    self.output_file.parent.mkdir(parents=True, exist_ok=True)\n\n    # Create and run the temporary sbatch file via slurm\n    with NamedTemporaryFile(\n        prefix=self.name, suffix=\".sbatch\", dir=Path(\"./\"), mode=\"w+\"\n    ) as sbatch_tmp:\n        sbatch_tmp.write(self.sbatch_contents)\n        sbatch_tmp.flush()\n        command_list = [\"sbatch\", Path(sbatch_tmp.name)]\n        if dependencies is not None:\n            dependencies_string = \",\".join(str(job_id) for job_id in dependencies)\n            command_list.insert(1, f\"--dependency=afterok:{dependencies_string}\")\n        try:\n            result = subprocess_run(  # nosec\n                command_list,  # type: ignore # noqa: S603, PGH003\n                check=True,\n                stdout=PIPE,\n            )\n        except CalledProcessError:\n            return None\n        job_id_search = re_search(SLURM_JOB_ID_REGEX, result.stdout.decode(\"utf-8\"))\n        if job_id_search is None:\n            return None\n        return int(job_id_search.group(1))\n</code></pre>"},{"location":"reference/run_configuration.html#hpc_multibench.run_configuration.RunConfiguration.collect","title":"<code>collect(slurm_id: int, check_queue: bool = False) -&gt; str | None</code>","text":"<p>Collect the output from a completed job with a given slurm id.</p> Source code in <code>src/hpc_multibench/run_configuration.py</code> <pre><code>def collect(\n    self, slurm_id: int, check_queue: bool = False  # noqa: FBT001, FBT002\n) -&gt; str | None:\n    \"\"\"Collect the output from a completed job with a given slurm id.\"\"\"\n    # Check the job is completed in the queue\n    if check_queue:\n        result = subprocess_run(  # nosec\n            [\"squeue\", \"-j\", str(slurm_id)],  # noqa: S603, S607\n            check=True,\n            stdout=PIPE,\n        )\n        if SLURM_UNQUEUED_SUBSTRING in result.stdout.decode(\"utf-8\"):\n            return None\n\n    # Return the contents of the specified output file\n    output_file = self.output_file.parent / self.get_true_output_file_name(slurm_id)\n    if not output_file.exists():\n        return None\n    return output_file.read_text(encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/run_configuration.html#hpc_multibench.run_configuration.RunConfiguration.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Get the sbatch configuration file defining the run.</p> Source code in <code>src/hpc_multibench/run_configuration.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the sbatch configuration file defining the run.\"\"\"\n    return self.sbatch_contents\n</code></pre>"},{"location":"reference/run_configuration.html#hpc_multibench.run_configuration.get_queued_job_ids","title":"<code>get_queued_job_ids() -&gt; list[int]</code>","text":"<p>Get the IDs of the jobs queued by the current user.</p> Source code in <code>src/hpc_multibench/run_configuration.py</code> <pre><code>def get_queued_job_ids() -&gt; list[int]:\n    \"\"\"Get the IDs of the jobs queued by the current user.\"\"\"\n    result = subprocess_run(  # nosec\n        [\"squeue\", \"-u\", getuser(), \"-o\", \"'%A'\", \"-h\"],  # noqa: S603, S607\n        check=True,\n        stdout=PIPE,\n    ).stdout.decode(\"utf-8\")\n    return [int(job_id.strip(\"'\")) for job_id in result.split(\"\\n\") if job_id != \"\"]\n</code></pre>"},{"location":"reference/test_bench.html","title":"Reference","text":""},{"location":"reference/test_bench.html#hpc_multibench.test_bench","title":"<code>test_bench</code>","text":"<p>A class representing a test bench composing part of a test plan.</p>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.RunConfigurationMetadata","title":"<code>RunConfigurationMetadata</code>  <code>dataclass</code>","text":"<p>Data about run configurations to persist between program instances.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>@dataclass(frozen=True)\nclass RunConfigurationMetadata:\n    \"\"\"Data about run configurations to persist between program instances.\"\"\"\n\n    job_id: int\n    rerun_count: int\n    name: str\n    output_file_name: str\n    instantiation: dict[str, Any] | None\n\n    def as_csv_row(self) -&gt; dict[str, Any]:\n        \"\"\"Get a representation of the data able to be written to a CSV file.\"\"\"\n        row = self.__dict__\n        row[\"instantiation\"] = b64encode(pickle_dumps(row[\"instantiation\"])).decode(\n            \"ascii\"\n        )\n        return row\n\n    @classmethod\n    def from_csv_row(cls, row: dict[str, Any]) -&gt; Self:\n        \"\"\"Construct the object from a representation usable in CSV files.\"\"\"\n        row[\"job_id\"] = int(row[\"job_id\"])\n        row[\"rerun_count\"] = int(row[\"rerun_count\"])\n        row[\"instantiation\"] = pickle_loads(  # noqa: S301 # nosec\n            b64decode(row[\"instantiation\"])\n        )\n        return cls(**row)\n\n    @classmethod\n    def fields(cls) -&gt; list[str]:\n        \"\"\"Return the field names of the dataclass.\"\"\"\n        return list(cls.__annotations__)\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.RunConfigurationMetadata.as_csv_row","title":"<code>as_csv_row() -&gt; dict[str, Any]</code>","text":"<p>Get a representation of the data able to be written to a CSV file.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>def as_csv_row(self) -&gt; dict[str, Any]:\n    \"\"\"Get a representation of the data able to be written to a CSV file.\"\"\"\n    row = self.__dict__\n    row[\"instantiation\"] = b64encode(pickle_dumps(row[\"instantiation\"])).decode(\n        \"ascii\"\n    )\n    return row\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.RunConfigurationMetadata.from_csv_row","title":"<code>from_csv_row(row: dict[str, Any]) -&gt; Self</code>  <code>classmethod</code>","text":"<p>Construct the object from a representation usable in CSV files.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>@classmethod\ndef from_csv_row(cls, row: dict[str, Any]) -&gt; Self:\n    \"\"\"Construct the object from a representation usable in CSV files.\"\"\"\n    row[\"job_id\"] = int(row[\"job_id\"])\n    row[\"rerun_count\"] = int(row[\"rerun_count\"])\n    row[\"instantiation\"] = pickle_loads(  # noqa: S301 # nosec\n        b64decode(row[\"instantiation\"])\n    )\n    return cls(**row)\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.RunConfigurationMetadata.fields","title":"<code>fields() -&gt; list[str]</code>  <code>classmethod</code>","text":"<p>Return the field names of the dataclass.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>@classmethod\ndef fields(cls) -&gt; list[str]:\n    \"\"\"Return the field names of the dataclass.\"\"\"\n    return list(cls.__annotations__)\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench","title":"<code>TestBench</code>","text":"<p>A test bench composing part of a test plan.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>class TestBench:\n    \"\"\"A test bench composing part of a test plan.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        run_configuration_models: dict[str, RunConfigurationModel],\n        bench_model: BenchModel,\n        base_output_directory: Path,\n    ) -&gt; None:\n        \"\"\"Instantiate the test bench.\"\"\"\n        self.name = name\n        self.run_configuration_models = run_configuration_models\n        self.bench_model = bench_model\n        self.base_output_directory = base_output_directory\n\n        # Validate that all configurations named in the test bench are defined\n        # in the test plan\n        for run_configuration_name in bench_model.run_configurations:\n            if run_configuration_name not in self.run_configuration_models:\n                raise RuntimeError(\n                    f\"'{run_configuration_name}' not in list of\"\n                    \" defined run configurations!\"\n                )\n\n    @property\n    def output_directory(self) -&gt; Path:\n        \"\"\"Get the output directory for the test bench.\"\"\"\n        return self.base_output_directory / self.name\n\n    @property\n    def instantiations(self) -&gt; list[dict[str, Any]]:\n        \"\"\"Get a list of run configuration instantiations from the test matrix.\"\"\"\n        shaped: list[list[list[tuple[str, Any]]]] = [\n            (\n                [[(key, value)] for value in values]\n                if isinstance(key, str)\n                else [list(zip(key, setting, strict=True)) for setting in values]\n            )\n            for key, values in self.bench_model.matrix.items()\n        ]\n        return [\n            {item[0]: item[1] for items in combination for item in items}\n            for combination in product(*shaped)\n        ]\n\n    @property\n    def _run_configurations_metadata_file(self) -&gt; Path:\n        \"\"\"Get the path to the file to save the run configuration metadata.\"\"\"\n        return self.output_directory / \"runs_metadata.csv\"\n\n    @property\n    def run_configurations_metadata(self) -&gt; list[RunConfigurationMetadata] | None:\n        \"\"\"Retrieve the run configuration metadata from its file.\"\"\"\n        if not self._run_configurations_metadata_file.exists():\n            return None\n\n        with self._run_configurations_metadata_file.open(\"r\") as metadata_file:\n            metadata_reader = DictReader(metadata_file)\n            return [\n                RunConfigurationMetadata.from_csv_row(row) for row in metadata_reader\n            ]\n\n    @run_configurations_metadata.setter\n    def run_configurations_metadata(\n        self, metadata: list[RunConfigurationMetadata]\n    ) -&gt; None:\n        \"\"\"\n        Write out the run configuration metadata to its file.\n\n        TODO: Consider whether this is actually right abstraction for passing\n        between program runs.\n        \"\"\"\n        existing_metadata = self.run_configurations_metadata\n        with self._run_configurations_metadata_file.open(\"w+\") as metadata_file:\n            metadata_writer = DictWriter(\n                metadata_file, fieldnames=RunConfigurationMetadata.fields()\n            )\n            metadata_writer.writeheader()\n            if existing_metadata is not None:\n                metadata_writer.writerows(\n                    item.as_csv_row() for item in existing_metadata\n                )\n            metadata_writer.writerows(item.as_csv_row() for item in metadata)\n\n    @property\n    def all_job_ids(self) -&gt; list[int]:\n        \"\"\"Get the total number of jobs submitted by the test bench.\"\"\"\n        run_configurations_metadata = self.run_configurations_metadata\n        if run_configurations_metadata is None:\n            return []\n        return [metadata.job_id for metadata in run_configurations_metadata]\n\n    def wait_for_queue(\n        self,\n        max_time_to_wait: int = 172_800,\n        backoff: list[int] | None = None,\n        verbose: bool = True,  # noqa: FBT001, FBT002\n    ) -&gt; bool:\n        \"\"\"Wait till the queue is drained of jobs submitted by this test bench.\"\"\"\n        if backoff is None or len(backoff) &lt; 1:\n            backoff = [5, 10, 15, 30, 60]\n\n        time_waited = 0\n        backoff_index = 0\n        while time_waited &lt; max_time_to_wait:\n            wait_time = backoff[backoff_index]\n            sleep(wait_time)\n            time_waited += wait_time\n\n            queued_jobs = set(get_queued_job_ids())\n            required_jobs = set(self.all_job_ids)\n            if len(required_jobs - queued_jobs) == len(required_jobs):\n                return False\n            if verbose:\n                print(\n                    f\"{len(required_jobs - queued_jobs)}/{len(required_jobs)} \"\n                    f\"jobs left for test bench '{self.name}'\"\n                )\n\n            if backoff_index &lt; len(backoff) - 1:\n                backoff_index += 1\n\n        return True\n\n    def record(self, args: Namespace) -&gt; None:\n        \"\"\"Spawn run configurations for the test bench.\"\"\"\n        # Optionally clobber directory\n        if not args.no_clobber and self.output_directory.exists():\n            rmtree(self.output_directory)\n\n        # Realise run configurations from list of instantiations, split up\n        # by model so they only get built once\n        realised_run_configurations: dict[str, list[RunConfiguration]] = {\n            run_name: [\n                deepcopy(run_model).realise(run_name, self.output_directory, instantiation)\n                for instantiation in self.instantiations\n            ]\n            for run_name, run_model in self.run_configuration_models.items()\n        }\n\n        # TODO: Need to account for case where build commands is in the\n        # matrix, then just needs to be a long chain of dependencies\n\n        # Run all run configurations and store their slurm job ids\n        run_configuration_job_ids: list[dict[int, RunConfiguration]] = []\n        dry_run_outputs: list[str] = []\n        for run_configurations in realised_run_configurations.values():\n            # Add dependencies on the first job of that run configuration, so\n            # you only need to build it once!\n            first_job_id: int | None = None\n            for run_configuration in run_configurations:\n                rerun_map: dict[int, RunConfiguration] = {}\n                for _ in range(self.bench_model.reruns.number):\n                    if first_job_id is None:\n                        if args.dry_run:\n                            dry_run_outputs.append(str(run_configuration))\n                            run_configuration.pre_built = True\n                            continue\n                        job_id = run_configuration.run()\n                        first_job_id = job_id\n                    else:\n                        if args.dry_run:\n                            dry_run_outputs.append(str(run_configuration))\n                            continue\n                        run_configuration.pre_built = True\n                        job_id = run_configuration.run(dependencies=[first_job_id])\n                    if job_id is None:\n                        print(\n                            f\"Run configuration '{run_configuration.name}' \"\n                            f\"with instantiation '{run_configuration.instantiation}' \"\n                            \"failed to queue!\"\n                        )\n                        continue\n                    rerun_map[job_id] = run_configuration\n                run_configuration_job_ids.append(rerun_map)\n\n        # Stop after printing the run configurations if dry running\n        if args.dry_run:\n            print(DRY_RUN_SEPARATOR.join(dry_run_outputs))\n            return\n\n        # Store slurm job id mappings, excluding ones which failed to launch\n        self.run_configurations_metadata = [\n            RunConfigurationMetadata(\n                job_id,\n                rerun_count,\n                run_configuration.name,\n                run_configuration.get_true_output_file_name(job_id),\n                run_configuration.instantiation,\n            )\n            for run_config_rerun_job_ids in run_configuration_job_ids\n            for rerun_count, (job_id, run_configuration) in enumerate(\n                run_config_rerun_job_ids.items()\n            )\n        ]\n\n    def extract_metrics(self, output: str) -&gt; dict[str, str] | None:\n        \"\"\"\n        Extract the specified metrics from the output file.\n\n        Note that run instantiations can be extracted via regex from output.\n        \"\"\"\n        metrics: dict[str, str] = {}\n        for metric, regex in self.bench_model.analysis.metrics.items():\n            metric_search = re_search(regex, output)\n            if metric_search is None:\n                return None\n            # TODO: Support multiple groups by lists as keys?\n            # NOTE: Strip commas to make it possible to parse numbers\n            metrics[metric] = metric_search.group(1).replace(\",\", \"\")\n        return metrics\n\n    def get_run_outputs(\n        self,\n    ) -&gt; list[dict[int, tuple[RunConfiguration, str | None]]] | None:\n        \"\"\"Get the outputs of the test bench runs.\"\"\"\n        if self.run_configurations_metadata is None:\n            print(f\"Metadata file does not exist for test bench '{self.name}'!\")\n            return None\n\n        # Reconstruct realised run configurations from the metadata file\n        reconstructed_run_configurations: list[dict[int, RunConfiguration]] = []\n        prev_rerun_count: int = -1\n        rerun_group: dict[int, RunConfiguration] = {}\n        for metadata in self.run_configurations_metadata:\n            # Split the data up by re-runs\n            if metadata.rerun_count != prev_rerun_count + 1 and len(rerun_group) &gt; 0:\n                reconstructed_run_configurations.append(rerun_group)\n                rerun_group = {}\n            prev_rerun_count = metadata.rerun_count\n\n            # Add the realised run configuration to its re-run dictionary\n            if metadata.name not in self.run_configuration_models:\n                # print(f\"Skipping {metadata.name} since excluded from YAML file.\")\n                continue\n            rerun_group[metadata.job_id] = self.run_configuration_models[\n                metadata.name\n            ].realise(metadata.name, self.output_directory, metadata.instantiation)\n        reconstructed_run_configurations.append(rerun_group)\n\n        # Collect outputs from the run configurations\n        run_outputs: list[dict[int, tuple[RunConfiguration, str | None]]] = [\n            {\n                job_id: (run_configuration, run_configuration.collect(job_id))\n                for job_id, run_configuration in rerun_group.items()\n            }\n            for rerun_group in reconstructed_run_configurations\n        ]\n\n        return run_outputs\n\n    def get_run_metrics(\n        self, run_outputs: list[dict[int, tuple[RunConfiguration, str | None]]]\n    ) -&gt; list[dict[int, tuple[RunConfiguration, dict[str, str]]]]:\n        \"\"\".\"\"\"\n        run_metrics: list[dict[int, tuple[RunConfiguration, dict[str, str]]]] = []\n        for rerun_group in run_outputs:\n            rerun_metrics: dict[int, tuple[RunConfiguration, dict[str, str]]] = {}\n            for job_id, (run_configuration, output) in rerun_group.items():\n                if output is None:\n                    print(\n                        f\"Run configuration '{run_configuration.name}'\"\n                        \" has no output!\"\n                    )\n                    continue\n\n                metrics = self.extract_metrics(output)\n                if metrics is None:\n                    print(\n                        \"Unable to extract metrics from run\"\n                        f\" configuration '{run_configuration.name}'!\"\n                    )\n                    continue\n\n                rerun_metrics[job_id] = (run_configuration, metrics)\n            run_metrics.append(rerun_metrics)\n        return run_metrics\n\n    def aggregate_run_metrics(  # noqa: C901\n        self, run_metrics: list[dict[int, tuple[RunConfiguration, dict[str, str]]]]\n    ) -&gt; list[tuple[RunConfiguration, dict[str, str | UFloat]]]:\n        \"\"\".\"\"\"\n        all_aggregated_metrics: list[\n            tuple[RunConfiguration, dict[str, str | UFloat]]\n        ] = []\n        for rerun_group in run_metrics:\n            # Get the mapping of metrics to their values across re-runs\n            canonical_run_configuration: RunConfiguration | None = None\n            grouped_metrics: dict[str, list[str]] = {}\n            for run_configuration, metrics in rerun_group.values():\n                if canonical_run_configuration is None:\n                    canonical_run_configuration = run_configuration\n\n                for metric, value in metrics.items():\n                    if metric not in grouped_metrics:\n                        grouped_metrics[metric] = []\n                    grouped_metrics[metric].append(value)\n\n            aggregated_metrics: dict[str, str | UFloat] = {}\n            reruns_model = self.bench_model.reruns\n            for metric, values in grouped_metrics.items():\n                # Just pick the first value of the metric if it cannot be\n                # aggregated\n                if (\n                    reruns_model.number == 1\n                    or metric in reruns_model.unaggregatable_metrics\n                ):\n                    aggregated_metrics[metric] = values[0]\n                    continue\n\n                # Remove highest then lowest in turn till depleted or one left\n                pruned_values: list[float] = sorted([float(value) for value in values])\n                highest_discard = reruns_model.highest_discard\n                lowest_discard = reruns_model.lowest_discard\n                while len(pruned_values) &gt; 1 and (\n                    highest_discard &gt; 0 or lowest_discard &gt; 0\n                ):\n                    if highest_discard &gt; 0:\n                        pruned_values = pruned_values[:-1]\n                        highest_discard -= 1\n                        if len(values) &lt;= 1:\n                            break\n                    if lowest_discard &gt; 0:\n                        pruned_values = pruned_values[1:]\n                        lowest_discard -= 1\n\n                # Take the average and standard deviation of the metrics\n                metric_mean = fmean(pruned_values)\n                metric_stdev = (\n                    stdev(pruned_values)\n                    if reruns_model.undiscarded_number &gt;= 2  # noqa: PLR2004\n                    else 0.0\n                )\n                aggregated_metrics[metric] = UFloat(metric_mean, metric_stdev)\n\n            # Update the metrics\n            if canonical_run_configuration is not None:\n                all_aggregated_metrics.append(\n                    (canonical_run_configuration, aggregated_metrics)\n                )\n\n        return all_aggregated_metrics\n\n    def calculate_derived_metrics(\n        self, input_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]\n    ) -&gt; list[tuple[RunConfiguration, dict[str, str | UFloat]]]:\n        \"\"\"Calculate derived metrics from definitions in the YAML file.\"\"\"\n        output_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]] = []\n\n        # {\"run configuration name\" : {\"run configuration name\": {\"instantation string\": \"instantation number\"}}}\n        instantiation_numbers: dict[str, dict[str, int]] = {}\n        # {\"run configuration name\" : {\"instantation #\": {\"metric\": \"value\"}}}\n        all_metrics: dict[str, dict[int, dict[str, str | UFloat]]] = {}\n        for run_configuration, metrics in input_metrics:\n            if run_configuration.instantiation is None:\n                continue\n            if run_configuration.name not in instantiation_numbers:\n                instantiation_numbers[run_configuration.name] = {}\n            instantiation_number = len(instantiation_numbers[run_configuration.name])\n            instantiation_numbers[run_configuration.name][\n                RunConfiguration.get_instantiation_repr(run_configuration.instantiation)\n            ] = instantiation_number\n\n            if run_configuration.name not in all_metrics:\n                all_metrics[run_configuration.name] = {}\n            all_metrics[run_configuration.name][instantiation_number] = metrics\n\n        for run_configuration, metrics in input_metrics:\n            if run_configuration.instantiation is None:\n                continue\n            instantiation_number = instantiation_numbers[run_configuration.name][\n                RunConfiguration.get_instantiation_repr(run_configuration.instantiation)\n            ]\n\n            # Present a helpful data structure for accessing other run configurations\n            # Comparisons are made instantiation-wise, so elise that variable\n            # {\"run configuration name\": {\"metric\": \"value\"}}\n            _corresponding_metrics: dict[str, dict[str, str | UFloat]] = {\n                run_configuration_name: instantiation_metrics\n                for (\n                    run_configuration_name,\n                    run_configuration_metrics,\n                ) in all_metrics.items()\n                for (\n                    iter_instantiation_number,\n                    instantiation_metrics,\n                ) in run_configuration_metrics.items()\n                if instantiation_number == iter_instantiation_number\n            }\n            # {\"instantiation #\": {\"metric\": \"value\"}}\n            _sequential_metrics: dict[int, dict[str, str | UFloat]] = all_metrics[\n                run_configuration.name\n            ]\n\n            for (\n                metric,\n                derivation,\n            ) in self.bench_model.analysis.derived_metrics.items():\n                value = eval(derivation)  # nosec: B307 # noqa: S307\n                if hasattr(value, \"nominal_value\") and hasattr(value, \"std_dev\"):\n                    value = UFloat(value.nominal_value, value.std_dev)\n                metrics[metric] = value\n            output_metrics.append((run_configuration, metrics))\n\n        return output_metrics\n\n    def report(self) -&gt; None:\n        \"\"\"Analyse completed run configurations for the test bench.\"\"\"\n        run_outputs = self.get_run_outputs()\n        if run_outputs is None:\n            return\n\n        run_metrics = self.get_run_metrics(run_outputs)\n        aggregated_metrics = self.aggregate_run_metrics(run_metrics)\n        derived_metrics = self.calculate_derived_metrics(aggregated_metrics)\n\n        # Draw the specified plots\n        for line_plot in self.bench_model.analysis.line_plots:\n            if not line_plot.enabled:\n                continue\n            draw_line_plot(line_plot, derived_metrics)\n\n        for bar_chart in self.bench_model.analysis.bar_charts:\n            if not bar_chart.enabled:\n                continue\n            draw_bar_chart(bar_chart, derived_metrics)\n\n        for roofline_plot in self.bench_model.analysis.roofline_plots:\n            if not roofline_plot.enabled:\n                continue\n            draw_roofline_plot(roofline_plot, derived_metrics)\n\n        for export_schema in self.bench_model.analysis.data_exports:\n            if not export_schema.enabled:\n                continue\n            export_data(export_schema, derived_metrics)\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.output_directory","title":"<code>output_directory: Path</code>  <code>property</code>","text":"<p>Get the output directory for the test bench.</p>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.instantiations","title":"<code>instantiations: list[dict[str, Any]]</code>  <code>property</code>","text":"<p>Get a list of run configuration instantiations from the test matrix.</p>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench._run_configurations_metadata_file","title":"<code>_run_configurations_metadata_file: Path</code>  <code>property</code>","text":"<p>Get the path to the file to save the run configuration metadata.</p>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.run_configurations_metadata","title":"<code>run_configurations_metadata: list[RunConfigurationMetadata] | None</code>  <code>property</code> <code>writable</code>","text":"<p>Retrieve the run configuration metadata from its file.</p>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.all_job_ids","title":"<code>all_job_ids: list[int]</code>  <code>property</code>","text":"<p>Get the total number of jobs submitted by the test bench.</p>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.__init__","title":"<code>__init__(name: str, run_configuration_models: dict[str, RunConfigurationModel], bench_model: BenchModel, base_output_directory: Path) -&gt; None</code>","text":"<p>Instantiate the test bench.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    run_configuration_models: dict[str, RunConfigurationModel],\n    bench_model: BenchModel,\n    base_output_directory: Path,\n) -&gt; None:\n    \"\"\"Instantiate the test bench.\"\"\"\n    self.name = name\n    self.run_configuration_models = run_configuration_models\n    self.bench_model = bench_model\n    self.base_output_directory = base_output_directory\n\n    # Validate that all configurations named in the test bench are defined\n    # in the test plan\n    for run_configuration_name in bench_model.run_configurations:\n        if run_configuration_name not in self.run_configuration_models:\n            raise RuntimeError(\n                f\"'{run_configuration_name}' not in list of\"\n                \" defined run configurations!\"\n            )\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.wait_for_queue","title":"<code>wait_for_queue(max_time_to_wait: int = 172800, backoff: list[int] | None = None, verbose: bool = True) -&gt; bool</code>","text":"<p>Wait till the queue is drained of jobs submitted by this test bench.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>def wait_for_queue(\n    self,\n    max_time_to_wait: int = 172_800,\n    backoff: list[int] | None = None,\n    verbose: bool = True,  # noqa: FBT001, FBT002\n) -&gt; bool:\n    \"\"\"Wait till the queue is drained of jobs submitted by this test bench.\"\"\"\n    if backoff is None or len(backoff) &lt; 1:\n        backoff = [5, 10, 15, 30, 60]\n\n    time_waited = 0\n    backoff_index = 0\n    while time_waited &lt; max_time_to_wait:\n        wait_time = backoff[backoff_index]\n        sleep(wait_time)\n        time_waited += wait_time\n\n        queued_jobs = set(get_queued_job_ids())\n        required_jobs = set(self.all_job_ids)\n        if len(required_jobs - queued_jobs) == len(required_jobs):\n            return False\n        if verbose:\n            print(\n                f\"{len(required_jobs - queued_jobs)}/{len(required_jobs)} \"\n                f\"jobs left for test bench '{self.name}'\"\n            )\n\n        if backoff_index &lt; len(backoff) - 1:\n            backoff_index += 1\n\n    return True\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.record","title":"<code>record(args: Namespace) -&gt; None</code>","text":"<p>Spawn run configurations for the test bench.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>def record(self, args: Namespace) -&gt; None:\n    \"\"\"Spawn run configurations for the test bench.\"\"\"\n    # Optionally clobber directory\n    if not args.no_clobber and self.output_directory.exists():\n        rmtree(self.output_directory)\n\n    # Realise run configurations from list of instantiations, split up\n    # by model so they only get built once\n    realised_run_configurations: dict[str, list[RunConfiguration]] = {\n        run_name: [\n            deepcopy(run_model).realise(run_name, self.output_directory, instantiation)\n            for instantiation in self.instantiations\n        ]\n        for run_name, run_model in self.run_configuration_models.items()\n    }\n\n    # TODO: Need to account for case where build commands is in the\n    # matrix, then just needs to be a long chain of dependencies\n\n    # Run all run configurations and store their slurm job ids\n    run_configuration_job_ids: list[dict[int, RunConfiguration]] = []\n    dry_run_outputs: list[str] = []\n    for run_configurations in realised_run_configurations.values():\n        # Add dependencies on the first job of that run configuration, so\n        # you only need to build it once!\n        first_job_id: int | None = None\n        for run_configuration in run_configurations:\n            rerun_map: dict[int, RunConfiguration] = {}\n            for _ in range(self.bench_model.reruns.number):\n                if first_job_id is None:\n                    if args.dry_run:\n                        dry_run_outputs.append(str(run_configuration))\n                        run_configuration.pre_built = True\n                        continue\n                    job_id = run_configuration.run()\n                    first_job_id = job_id\n                else:\n                    if args.dry_run:\n                        dry_run_outputs.append(str(run_configuration))\n                        continue\n                    run_configuration.pre_built = True\n                    job_id = run_configuration.run(dependencies=[first_job_id])\n                if job_id is None:\n                    print(\n                        f\"Run configuration '{run_configuration.name}' \"\n                        f\"with instantiation '{run_configuration.instantiation}' \"\n                        \"failed to queue!\"\n                    )\n                    continue\n                rerun_map[job_id] = run_configuration\n            run_configuration_job_ids.append(rerun_map)\n\n    # Stop after printing the run configurations if dry running\n    if args.dry_run:\n        print(DRY_RUN_SEPARATOR.join(dry_run_outputs))\n        return\n\n    # Store slurm job id mappings, excluding ones which failed to launch\n    self.run_configurations_metadata = [\n        RunConfigurationMetadata(\n            job_id,\n            rerun_count,\n            run_configuration.name,\n            run_configuration.get_true_output_file_name(job_id),\n            run_configuration.instantiation,\n        )\n        for run_config_rerun_job_ids in run_configuration_job_ids\n        for rerun_count, (job_id, run_configuration) in enumerate(\n            run_config_rerun_job_ids.items()\n        )\n    ]\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.extract_metrics","title":"<code>extract_metrics(output: str) -&gt; dict[str, str] | None</code>","text":"<p>Extract the specified metrics from the output file.</p> <p>Note that run instantiations can be extracted via regex from output.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>def extract_metrics(self, output: str) -&gt; dict[str, str] | None:\n    \"\"\"\n    Extract the specified metrics from the output file.\n\n    Note that run instantiations can be extracted via regex from output.\n    \"\"\"\n    metrics: dict[str, str] = {}\n    for metric, regex in self.bench_model.analysis.metrics.items():\n        metric_search = re_search(regex, output)\n        if metric_search is None:\n            return None\n        # TODO: Support multiple groups by lists as keys?\n        # NOTE: Strip commas to make it possible to parse numbers\n        metrics[metric] = metric_search.group(1).replace(\",\", \"\")\n    return metrics\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.get_run_outputs","title":"<code>get_run_outputs() -&gt; list[dict[int, tuple[RunConfiguration, str | None]]] | None</code>","text":"<p>Get the outputs of the test bench runs.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>def get_run_outputs(\n    self,\n) -&gt; list[dict[int, tuple[RunConfiguration, str | None]]] | None:\n    \"\"\"Get the outputs of the test bench runs.\"\"\"\n    if self.run_configurations_metadata is None:\n        print(f\"Metadata file does not exist for test bench '{self.name}'!\")\n        return None\n\n    # Reconstruct realised run configurations from the metadata file\n    reconstructed_run_configurations: list[dict[int, RunConfiguration]] = []\n    prev_rerun_count: int = -1\n    rerun_group: dict[int, RunConfiguration] = {}\n    for metadata in self.run_configurations_metadata:\n        # Split the data up by re-runs\n        if metadata.rerun_count != prev_rerun_count + 1 and len(rerun_group) &gt; 0:\n            reconstructed_run_configurations.append(rerun_group)\n            rerun_group = {}\n        prev_rerun_count = metadata.rerun_count\n\n        # Add the realised run configuration to its re-run dictionary\n        if metadata.name not in self.run_configuration_models:\n            # print(f\"Skipping {metadata.name} since excluded from YAML file.\")\n            continue\n        rerun_group[metadata.job_id] = self.run_configuration_models[\n            metadata.name\n        ].realise(metadata.name, self.output_directory, metadata.instantiation)\n    reconstructed_run_configurations.append(rerun_group)\n\n    # Collect outputs from the run configurations\n    run_outputs: list[dict[int, tuple[RunConfiguration, str | None]]] = [\n        {\n            job_id: (run_configuration, run_configuration.collect(job_id))\n            for job_id, run_configuration in rerun_group.items()\n        }\n        for rerun_group in reconstructed_run_configurations\n    ]\n\n    return run_outputs\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.get_run_metrics","title":"<code>get_run_metrics(run_outputs: list[dict[int, tuple[RunConfiguration, str | None]]]) -&gt; list[dict[int, tuple[RunConfiguration, dict[str, str]]]]</code>","text":"<p>.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>def get_run_metrics(\n    self, run_outputs: list[dict[int, tuple[RunConfiguration, str | None]]]\n) -&gt; list[dict[int, tuple[RunConfiguration, dict[str, str]]]]:\n    \"\"\".\"\"\"\n    run_metrics: list[dict[int, tuple[RunConfiguration, dict[str, str]]]] = []\n    for rerun_group in run_outputs:\n        rerun_metrics: dict[int, tuple[RunConfiguration, dict[str, str]]] = {}\n        for job_id, (run_configuration, output) in rerun_group.items():\n            if output is None:\n                print(\n                    f\"Run configuration '{run_configuration.name}'\"\n                    \" has no output!\"\n                )\n                continue\n\n            metrics = self.extract_metrics(output)\n            if metrics is None:\n                print(\n                    \"Unable to extract metrics from run\"\n                    f\" configuration '{run_configuration.name}'!\"\n                )\n                continue\n\n            rerun_metrics[job_id] = (run_configuration, metrics)\n        run_metrics.append(rerun_metrics)\n    return run_metrics\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.aggregate_run_metrics","title":"<code>aggregate_run_metrics(run_metrics: list[dict[int, tuple[RunConfiguration, dict[str, str]]]]) -&gt; list[tuple[RunConfiguration, dict[str, str | UFloat]]]</code>","text":"<p>.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>def aggregate_run_metrics(  # noqa: C901\n    self, run_metrics: list[dict[int, tuple[RunConfiguration, dict[str, str]]]]\n) -&gt; list[tuple[RunConfiguration, dict[str, str | UFloat]]]:\n    \"\"\".\"\"\"\n    all_aggregated_metrics: list[\n        tuple[RunConfiguration, dict[str, str | UFloat]]\n    ] = []\n    for rerun_group in run_metrics:\n        # Get the mapping of metrics to their values across re-runs\n        canonical_run_configuration: RunConfiguration | None = None\n        grouped_metrics: dict[str, list[str]] = {}\n        for run_configuration, metrics in rerun_group.values():\n            if canonical_run_configuration is None:\n                canonical_run_configuration = run_configuration\n\n            for metric, value in metrics.items():\n                if metric not in grouped_metrics:\n                    grouped_metrics[metric] = []\n                grouped_metrics[metric].append(value)\n\n        aggregated_metrics: dict[str, str | UFloat] = {}\n        reruns_model = self.bench_model.reruns\n        for metric, values in grouped_metrics.items():\n            # Just pick the first value of the metric if it cannot be\n            # aggregated\n            if (\n                reruns_model.number == 1\n                or metric in reruns_model.unaggregatable_metrics\n            ):\n                aggregated_metrics[metric] = values[0]\n                continue\n\n            # Remove highest then lowest in turn till depleted or one left\n            pruned_values: list[float] = sorted([float(value) for value in values])\n            highest_discard = reruns_model.highest_discard\n            lowest_discard = reruns_model.lowest_discard\n            while len(pruned_values) &gt; 1 and (\n                highest_discard &gt; 0 or lowest_discard &gt; 0\n            ):\n                if highest_discard &gt; 0:\n                    pruned_values = pruned_values[:-1]\n                    highest_discard -= 1\n                    if len(values) &lt;= 1:\n                        break\n                if lowest_discard &gt; 0:\n                    pruned_values = pruned_values[1:]\n                    lowest_discard -= 1\n\n            # Take the average and standard deviation of the metrics\n            metric_mean = fmean(pruned_values)\n            metric_stdev = (\n                stdev(pruned_values)\n                if reruns_model.undiscarded_number &gt;= 2  # noqa: PLR2004\n                else 0.0\n            )\n            aggregated_metrics[metric] = UFloat(metric_mean, metric_stdev)\n\n        # Update the metrics\n        if canonical_run_configuration is not None:\n            all_aggregated_metrics.append(\n                (canonical_run_configuration, aggregated_metrics)\n            )\n\n    return all_aggregated_metrics\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.calculate_derived_metrics","title":"<code>calculate_derived_metrics(input_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]) -&gt; list[tuple[RunConfiguration, dict[str, str | UFloat]]]</code>","text":"<p>Calculate derived metrics from definitions in the YAML file.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>def calculate_derived_metrics(\n    self, input_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]]\n) -&gt; list[tuple[RunConfiguration, dict[str, str | UFloat]]]:\n    \"\"\"Calculate derived metrics from definitions in the YAML file.\"\"\"\n    output_metrics: list[tuple[RunConfiguration, dict[str, str | UFloat]]] = []\n\n    # {\"run configuration name\" : {\"run configuration name\": {\"instantation string\": \"instantation number\"}}}\n    instantiation_numbers: dict[str, dict[str, int]] = {}\n    # {\"run configuration name\" : {\"instantation #\": {\"metric\": \"value\"}}}\n    all_metrics: dict[str, dict[int, dict[str, str | UFloat]]] = {}\n    for run_configuration, metrics in input_metrics:\n        if run_configuration.instantiation is None:\n            continue\n        if run_configuration.name not in instantiation_numbers:\n            instantiation_numbers[run_configuration.name] = {}\n        instantiation_number = len(instantiation_numbers[run_configuration.name])\n        instantiation_numbers[run_configuration.name][\n            RunConfiguration.get_instantiation_repr(run_configuration.instantiation)\n        ] = instantiation_number\n\n        if run_configuration.name not in all_metrics:\n            all_metrics[run_configuration.name] = {}\n        all_metrics[run_configuration.name][instantiation_number] = metrics\n\n    for run_configuration, metrics in input_metrics:\n        if run_configuration.instantiation is None:\n            continue\n        instantiation_number = instantiation_numbers[run_configuration.name][\n            RunConfiguration.get_instantiation_repr(run_configuration.instantiation)\n        ]\n\n        # Present a helpful data structure for accessing other run configurations\n        # Comparisons are made instantiation-wise, so elise that variable\n        # {\"run configuration name\": {\"metric\": \"value\"}}\n        _corresponding_metrics: dict[str, dict[str, str | UFloat]] = {\n            run_configuration_name: instantiation_metrics\n            for (\n                run_configuration_name,\n                run_configuration_metrics,\n            ) in all_metrics.items()\n            for (\n                iter_instantiation_number,\n                instantiation_metrics,\n            ) in run_configuration_metrics.items()\n            if instantiation_number == iter_instantiation_number\n        }\n        # {\"instantiation #\": {\"metric\": \"value\"}}\n        _sequential_metrics: dict[int, dict[str, str | UFloat]] = all_metrics[\n            run_configuration.name\n        ]\n\n        for (\n            metric,\n            derivation,\n        ) in self.bench_model.analysis.derived_metrics.items():\n            value = eval(derivation)  # nosec: B307 # noqa: S307\n            if hasattr(value, \"nominal_value\") and hasattr(value, \"std_dev\"):\n                value = UFloat(value.nominal_value, value.std_dev)\n            metrics[metric] = value\n        output_metrics.append((run_configuration, metrics))\n\n    return output_metrics\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.test_bench.TestBench.report","title":"<code>report() -&gt; None</code>","text":"<p>Analyse completed run configurations for the test bench.</p> Source code in <code>src/hpc_multibench/test_bench.py</code> <pre><code>def report(self) -&gt; None:\n    \"\"\"Analyse completed run configurations for the test bench.\"\"\"\n    run_outputs = self.get_run_outputs()\n    if run_outputs is None:\n        return\n\n    run_metrics = self.get_run_metrics(run_outputs)\n    aggregated_metrics = self.aggregate_run_metrics(run_metrics)\n    derived_metrics = self.calculate_derived_metrics(aggregated_metrics)\n\n    # Draw the specified plots\n    for line_plot in self.bench_model.analysis.line_plots:\n        if not line_plot.enabled:\n            continue\n        draw_line_plot(line_plot, derived_metrics)\n\n    for bar_chart in self.bench_model.analysis.bar_charts:\n        if not bar_chart.enabled:\n            continue\n        draw_bar_chart(bar_chart, derived_metrics)\n\n    for roofline_plot in self.bench_model.analysis.roofline_plots:\n        if not roofline_plot.enabled:\n            continue\n        draw_roofline_plot(roofline_plot, derived_metrics)\n\n    for export_schema in self.bench_model.analysis.data_exports:\n        if not export_schema.enabled:\n            continue\n        export_data(export_schema, derived_metrics)\n</code></pre>"},{"location":"reference/test_bench.html#hpc_multibench.uncertainties","title":"<code>uncertainties</code>","text":"<p>Export a set of functions for representing values with uncertainties.</p>"},{"location":"reference/tui.html","title":"Reference","text":""},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui","title":"<code>interactive_ui</code>","text":"<p>The definition of the interactive user interface.</p>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.ShowMode","title":"<code>ShowMode</code>","text":"<p>             Bases: <code>Enum</code></p> <p>The current state of the application.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>class ShowMode(Enum):\n    \"\"\"The current state of the application.\"\"\"\n\n    TestBench = auto()\n    RunConfiguration = auto()\n    Uninitialised = auto()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.TestPlanTree","title":"<code>TestPlanTree</code>","text":"<p>             Bases: <code>Tree[TestPlanTreeType]</code></p> <p>A tree showing the hierarchy of benches and runs in a test plan.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>class TestPlanTree(Tree[TestPlanTreeType]):\n    \"\"\"A tree showing the hierarchy of benches and runs in a test plan.\"\"\"\n\n    def __init__(self, *args, **kwargs) -&gt; None:  # type: ignore[no-untyped-def]\n        \"\"\"Instantiate a tree representing a test plan.\"\"\"\n        self.previous_cursor_node: TreeNode[TestPlanTreeType] | None = None\n        self._app: UserInterface = self.app  # type: ignore[assignment]\n        super().__init__(*args, **kwargs)\n\n    def populate(self) -&gt; None:\n        \"\"\"Populate the tree with data from the test plan.\"\"\"\n        self.clear()\n        for bench in self._app.test_plan.benches:\n            bench_node = self.root.add(\n                (\n                    bench.name\n                    if bench.bench_model.enabled\n                    else f\"[dim]{bench.name}[/dim]\"\n                ),\n                data=bench,\n            )\n            for (\n                run_configuration_name,\n                run_configuration,\n            ) in bench.run_configuration_models.items():\n                bench_node.add(\n                    (\n                        run_configuration_name\n                        if bench.bench_model.enabled\n                        else f\"[dim]{run_configuration_name}[/dim]\"\n                    ),\n                    allow_expand=False,\n                    data=run_configuration,\n                )\n            if bench.bench_model.enabled:\n                bench_node.expand()\n        self.root.expand()\n\n    def action_select_cursor(self) -&gt; None:\n        \"\"\"Pass the selection back and only toggle if already selected.\"\"\"\n        if self.cursor_node is not None:\n            self._app.handle_tree_selection(self.cursor_node)\n            if self.cursor_node in (self.previous_cursor_node, self.root):\n                self.cursor_node.toggle()\n        self.previous_cursor_node = self.cursor_node\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.TestPlanTree.__init__","title":"<code>__init__(*args, **kwargs) -&gt; None</code>","text":"<p>Instantiate a tree representing a test plan.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:  # type: ignore[no-untyped-def]\n    \"\"\"Instantiate a tree representing a test plan.\"\"\"\n    self.previous_cursor_node: TreeNode[TestPlanTreeType] | None = None\n    self._app: UserInterface = self.app  # type: ignore[assignment]\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.TestPlanTree.populate","title":"<code>populate() -&gt; None</code>","text":"<p>Populate the tree with data from the test plan.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def populate(self) -&gt; None:\n    \"\"\"Populate the tree with data from the test plan.\"\"\"\n    self.clear()\n    for bench in self._app.test_plan.benches:\n        bench_node = self.root.add(\n            (\n                bench.name\n                if bench.bench_model.enabled\n                else f\"[dim]{bench.name}[/dim]\"\n            ),\n            data=bench,\n        )\n        for (\n            run_configuration_name,\n            run_configuration,\n        ) in bench.run_configuration_models.items():\n            bench_node.add(\n                (\n                    run_configuration_name\n                    if bench.bench_model.enabled\n                    else f\"[dim]{run_configuration_name}[/dim]\"\n                ),\n                allow_expand=False,\n                data=run_configuration,\n            )\n        if bench.bench_model.enabled:\n            bench_node.expand()\n    self.root.expand()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.TestPlanTree.action_select_cursor","title":"<code>action_select_cursor() -&gt; None</code>","text":"<p>Pass the selection back and only toggle if already selected.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def action_select_cursor(self) -&gt; None:\n    \"\"\"Pass the selection back and only toggle if already selected.\"\"\"\n    if self.cursor_node is not None:\n        self._app.handle_tree_selection(self.cursor_node)\n        if self.cursor_node in (self.previous_cursor_node, self.root):\n            self.cursor_node.toggle()\n    self.previous_cursor_node = self.cursor_node\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.RunDialogScreen","title":"<code>RunDialogScreen</code>","text":"<p>             Bases: <code>Screen[None]</code></p> <p>Screen with a dialog to quit.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>class RunDialogScreen(Screen[None]):\n    \"\"\"Screen with a dialog to quit.\"\"\"\n\n    progress_timer: Timer\n\n    def __init__(self, *args, **kwargs) -&gt; None:  # type: ignore[no-untyped-def]\n        \"\"\"Instantiate a dialog screen for spawning runs.\"\"\"\n        self._app: UserInterface = self.app  # type: ignore[assignment]\n        self.jobs_spawned: bool = False\n        super().__init__(*args, **kwargs)\n\n    def compose(self) -&gt; ComposeResult:\n        \"\"\"Compose the structure of the dialog screen.\"\"\"\n        with Center(id=\"run-dialog\"):\n            yield Label(\n                (\n                    \"[bold]Waiting for queued jobs to complete.[/bold]\\n\\n\"\n                    \"You can continue, but will need to reload the test plan \"\n                    \"once they are complete to see any new results.\"\n                ),\n                id=\"run-dialog-message\",\n            )\n            yield ProgressBar(id=\"run-dialog-progress\")\n            yield Button(\"Continue\", variant=\"primary\", id=\"run-dialog-continue\")\n\n    def on_mount(self) -&gt; None:\n        \"\"\"Set up a timer to simulate progess happening.\"\"\"\n        self.progress_timer = self.set_interval(5, self.make_progress)\n\n    def make_progress(self) -&gt; None:\n        \"\"\"Automatically advance the progress bar.\"\"\"\n        progress_bar = self.query_one(\"#run-dialog-progress\", ProgressBar)\n        if not self.jobs_spawned:\n            self.jobs_spawned = True\n            # Wait till everything is rendered to kick off blocking calls...\n            for bench in self._app.test_plan.benches:\n                if bench.bench_model.enabled:\n                    bench.record(self._app.command_args)\n            total_jobs = sum(\n                [\n                    len(set(bench.all_job_ids))\n                    for bench in self._app.test_plan.benches\n                    if bench.bench_model.enabled\n                ]\n            )\n            progress_bar.update(total=total_jobs)\n\n        # Update the progress based on jobs completed\n        queued_jobs = set(get_queued_job_ids())\n        completed_jobs = sum(\n            [\n                len(set(bench.all_job_ids) - queued_jobs)\n                for bench in self._app.test_plan.benches\n                if bench.bench_model.enabled\n            ]\n        )\n        progress_bar.progress = completed_jobs\n\n        # Reload the test plan when all the jobs are completed\n        if progress_bar.progress == progress_bar.total:\n            self.query_one(\"#run-dialog-message\", Label).update(\n                \"[bold]All queued jobs have completed!\\n\\n[/bold]\"\n                \"Press 'Continue' to dismiss this dialog, then 'R' on the \"\n                \"keyboard to reload the test plan and view the new results.\"\n            )\n\n    def on_button_pressed(self, event: Button.Pressed) -&gt; None:\n        \"\"\"Dismiss the modal dialog when the continue button is pressed.\"\"\"\n        if event.button.id == \"run-dialog-continue\":\n            self.progress_timer.stop()\n            self.app.pop_screen()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.RunDialogScreen.__init__","title":"<code>__init__(*args, **kwargs) -&gt; None</code>","text":"<p>Instantiate a dialog screen for spawning runs.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:  # type: ignore[no-untyped-def]\n    \"\"\"Instantiate a dialog screen for spawning runs.\"\"\"\n    self._app: UserInterface = self.app  # type: ignore[assignment]\n    self.jobs_spawned: bool = False\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.RunDialogScreen.compose","title":"<code>compose() -&gt; ComposeResult</code>","text":"<p>Compose the structure of the dialog screen.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def compose(self) -&gt; ComposeResult:\n    \"\"\"Compose the structure of the dialog screen.\"\"\"\n    with Center(id=\"run-dialog\"):\n        yield Label(\n            (\n                \"[bold]Waiting for queued jobs to complete.[/bold]\\n\\n\"\n                \"You can continue, but will need to reload the test plan \"\n                \"once they are complete to see any new results.\"\n            ),\n            id=\"run-dialog-message\",\n        )\n        yield ProgressBar(id=\"run-dialog-progress\")\n        yield Button(\"Continue\", variant=\"primary\", id=\"run-dialog-continue\")\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.RunDialogScreen.on_mount","title":"<code>on_mount() -&gt; None</code>","text":"<p>Set up a timer to simulate progess happening.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def on_mount(self) -&gt; None:\n    \"\"\"Set up a timer to simulate progess happening.\"\"\"\n    self.progress_timer = self.set_interval(5, self.make_progress)\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.RunDialogScreen.make_progress","title":"<code>make_progress() -&gt; None</code>","text":"<p>Automatically advance the progress bar.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def make_progress(self) -&gt; None:\n    \"\"\"Automatically advance the progress bar.\"\"\"\n    progress_bar = self.query_one(\"#run-dialog-progress\", ProgressBar)\n    if not self.jobs_spawned:\n        self.jobs_spawned = True\n        # Wait till everything is rendered to kick off blocking calls...\n        for bench in self._app.test_plan.benches:\n            if bench.bench_model.enabled:\n                bench.record(self._app.command_args)\n        total_jobs = sum(\n            [\n                len(set(bench.all_job_ids))\n                for bench in self._app.test_plan.benches\n                if bench.bench_model.enabled\n            ]\n        )\n        progress_bar.update(total=total_jobs)\n\n    # Update the progress based on jobs completed\n    queued_jobs = set(get_queued_job_ids())\n    completed_jobs = sum(\n        [\n            len(set(bench.all_job_ids) - queued_jobs)\n            for bench in self._app.test_plan.benches\n            if bench.bench_model.enabled\n        ]\n    )\n    progress_bar.progress = completed_jobs\n\n    # Reload the test plan when all the jobs are completed\n    if progress_bar.progress == progress_bar.total:\n        self.query_one(\"#run-dialog-message\", Label).update(\n            \"[bold]All queued jobs have completed!\\n\\n[/bold]\"\n            \"Press 'Continue' to dismiss this dialog, then 'R' on the \"\n            \"keyboard to reload the test plan and view the new results.\"\n        )\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.RunDialogScreen.on_button_pressed","title":"<code>on_button_pressed(event: Button.Pressed) -&gt; None</code>","text":"<p>Dismiss the modal dialog when the continue button is pressed.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def on_button_pressed(self, event: Button.Pressed) -&gt; None:\n    \"\"\"Dismiss the modal dialog when the continue button is pressed.\"\"\"\n    if event.button.id == \"run-dialog-continue\":\n        self.progress_timer.stop()\n        self.app.pop_screen()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface","title":"<code>UserInterface</code>","text":"<p>             Bases: <code>App[None]</code></p> <p>The interactive TUI.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>class UserInterface(App[None]):\n    \"\"\"The interactive TUI.\"\"\"\n\n    CSS_PATH = \"interactive_ui.tcss\"\n    TITLE = \"HPC MultiBench\"\n    SUB_TITLE = \"A Swiss army knife for comparing programs on HPC resources\"\n\n    BINDINGS = [\n        (\"r\", \"reload_test_plan()\", \"Reload Test Plan\"),\n        # (\"s\", \"sort_selected_column()\", \"Sort by selected column\"),\n        (\"n\", \"change_plot(1)\", \"Next Graph\"),\n        (\"m\", \"change_plot(-1)\", \"Previous Graph\"),\n        (\"p\", \"open_graph()\", \"Open Graph\"),\n        (\"q\", \"quit\", \"Quit\"),\n    ]\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self, test_plan: TestPlan, command_args: Namespace, *args, **kwargs\n    ) -&gt; None:\n        \"\"\"Initialise the user interface.\"\"\"\n        self.test_plan: TestPlan = test_plan\n        self.command_args: Namespace = command_args\n        self.show_mode = ShowMode.Uninitialised\n        self.current_test_bench: TestBench | None = None\n        self.current_run_configuration: RunConfigurationModel | None = None\n        self.current_run_configuration_name: str | None = None\n        self.current_plot_index: int | None = None\n        super().__init__(*args, **kwargs)\n\n    def compose(self) -&gt; ComposeResult:\n        \"\"\"Compose the structure of the application.\"\"\"\n        yield Header()\n        with Horizontal():\n            # The navigation bar for the test plan\n            with Vertical(id=\"explorer\"):\n                yield Button(\"Run Test Plan\", id=\"run-button\")\n                yield TestPlanTree(label=\"Test Plan\", id=\"tree-explorer\")\n\n            # The starting pane that conceals the data pane when nothing selected\n            with Container(id=\"start-pane\"):\n                yield Label(\"Select a benchmark or run to start\", id=\"start-pane-label\")\n\n            with TabbedContent(initial=INITIAL_TAB, id=\"informer\"):\n                with TabPane(\"Run\", id=\"run-tab\"):\n                    yield DataTable(id=\"run-information\")\n                    # TODO: Get bash language working\n                    yield TextArea(\n                        \"echo hello\",\n                        id=\"sbatch-contents\",\n                        read_only=True,\n                        show_line_numbers=True,\n                        theme=\"monokai\",\n                    )\n                with TabPane(\"Metrics\", id=\"metrics-tab\"):\n                    yield DataTable(id=\"metrics-table\")\n                with TabPane(\"Plot\", id=\"plot-tab\"):\n                    yield PlotextPlot(id=\"metrics-plot\")\n        yield Footer()\n\n    def on_mount(self) -&gt; None:\n        \"\"\"Initialise data when the application is created.\"\"\"\n        self.initialise_test_plan_tree()\n        self.query_one(\"#sbatch-contents\", TextArea).register_language(\n            get_language(\"bash\"), BASH_HIGHLIGHTS\n        )\n        self.query_one(\"#sbatch-contents\", TextArea).language = \"bash\"\n\n    def on_button_pressed(self, event: Button.Pressed) -&gt; None:\n        \"\"\"When a button is pressed.\"\"\"\n        if event.button.id == \"run-button\":\n            self.push_screen(RunDialogScreen())\n\n    def on_data_table_cell_selected(self, event: DataTable.CellSelected) -&gt; None:\n        \"\"\"When a data table cell is selected.\"\"\"\n        if event.data_table.id == \"run-information\":\n            self.update_sbatch_contents()\n\n    def remove_start_pane(self) -&gt; None:\n        \"\"\"Remove the start pane from the screen if it is uninitialised.\"\"\"\n        if self.show_mode is ShowMode.Uninitialised:\n            self.query_one(\"#start-pane\", Container).remove()\n\n    def initialise_test_plan_tree(self) -&gt; None:\n        \"\"\"Initialise the test plan tree.\"\"\"\n        tree = self.query_one(TestPlanTree)\n        tree.populate()\n        self.app.set_focus(tree)\n\n    def handle_tree_selection(self, node: TreeNode[TestPlanTreeType]) -&gt; None:\n        \"\"\"Drive the user interface updates when new tree nodes are selected.\"\"\"\n        if node == self.query_one(TestPlanTree).root:\n            return\n\n        self.remove_start_pane()\n\n        if isinstance(node.data, TestBench):\n            self.show_mode = ShowMode.TestBench\n            self.current_test_bench = node.data\n            self.current_run_configuration = None\n            self.current_run_configuration_name = None\n        elif isinstance(node.data, RunConfigurationModel):\n            self.show_mode = ShowMode.RunConfiguration\n            assert node.parent is not None\n            self.current_test_bench = cast(TestBench, node.parent.data)\n            self.current_run_configuration = node.data\n            self.current_run_configuration_name = (\n                str(node.label).strip(\"[dim]\").strip(\"[/dim]\")\n            )\n\n        self.current_plot_index = 0\n        self.update_all_tabs()\n\n    def update_all_tabs(self) -&gt; None:\n        \"\"\"Update all tabs in the user interface.\"\"\"\n        self.update_run_tab()\n        self.update_metrics_tab()\n        self.update_plot_tab()\n\n    def update_run_tab(self) -&gt; None:\n        \"\"\"Update the run tab of the user interface.\"\"\"\n        self.update_run_information()\n        self.update_sbatch_contents()\n\n    def update_run_information(self) -&gt; None:\n        \"\"\"Update the instantiations table in the run tab.\"\"\"\n        run_information = self.query_one(\"#run-information\", DataTable)\n\n        assert self.current_test_bench is not None\n        instantiations = self.current_test_bench.instantiations\n\n        run_information.clear(columns=True)\n        if len(instantiations) &gt; 0:\n            run_information.add_columns(*instantiations[0].keys())\n        for instantiation in instantiations:\n            run_information.add_row(*instantiation.values())\n\n    def update_sbatch_contents(self) -&gt; None:\n        \"\"\"Update the sbatch contents in the run tab.\"\"\"\n        run_information = self.query_one(\"#run-information\", DataTable)\n        sbatch_contents = self.query_one(\"#sbatch-contents\", TextArea)\n\n        assert self.current_test_bench is not None\n        instantiations = self.current_test_bench.instantiations\n        if self.show_mode == ShowMode.TestBench:\n            sbatch_contents.visible = False\n            sbatch_contents.text = \"\"\n        else:\n            assert self.current_run_configuration is not None\n            assert self.current_run_configuration_name is not None\n            sbatch_contents.visible = True\n            selected_instantiation = instantiations[run_information.cursor_row]\n\n            sbatch_contents.text = self.current_run_configuration.realise(\n                self.current_run_configuration_name,\n                self.current_test_bench.output_directory,\n                selected_instantiation,\n            ).sbatch_contents\n\n    def get_aggregated_metrics(\n        self,\n    ) -&gt; list[tuple[RunConfiguration, dict[str, str | UFloat]]] | None:\n        \"\"\".\"\"\"\n        assert self.current_test_bench is not None\n        run_outputs = self.current_test_bench.get_run_outputs()\n        if run_outputs is None:\n            return None\n        run_metrics = self.current_test_bench.get_run_metrics(run_outputs)\n        aggregated_metrics = self.current_test_bench.aggregate_run_metrics(run_metrics)\n        return self.current_test_bench.calculate_derived_metrics(aggregated_metrics)\n\n    def update_metrics_tab(self) -&gt; None:\n        \"\"\"Update the metrics tab of the user interface.\"\"\"\n        metrics_table = self.query_one(\"#metrics-table\", DataTable)\n        metrics_table.clear(columns=True)\n\n        assert self.current_test_bench is not None\n        aggregated_metrics = self.get_aggregated_metrics()\n        if aggregated_metrics is None:\n            metrics_table.add_columns(\"No run data to show!\")\n            return\n\n        if self.show_mode == ShowMode.TestBench:\n            metrics_table.add_columns(\n                \"Name\",\n                *list(self.current_test_bench.bench_model.analysis.metrics.keys()),\n                *list(\n                    self.current_test_bench.bench_model.analysis.derived_metrics.keys()\n                ),\n            )\n            for run_configuration, metrics in aggregated_metrics:\n                metrics_table.add_row(\n                    run_configuration.name,\n                    *list(metrics.values()),\n                )\n        else:\n            assert self.current_run_configuration is not None\n            assert self.current_run_configuration_name is not None\n            metrics_table.add_columns(\n                *list(self.current_test_bench.bench_model.analysis.metrics.keys()),\n                *list(\n                    self.current_test_bench.bench_model.analysis.derived_metrics.keys()\n                ),\n            )\n            for run_configuration, metrics in aggregated_metrics:\n                if run_configuration.name != self.current_run_configuration_name:\n                    continue\n                metrics_table.add_row(*list(metrics.values()))\n\n    def update_plot_tab(self) -&gt; None:\n        \"\"\"Update the plot tab of the user interface.\"\"\"\n        metrics_plot_widget = self.query_one(\"#metrics-plot\", PlotextPlot)\n        metrics_plot = metrics_plot_widget.plt\n\n        assert self.current_test_bench is not None\n        aggregated_metrics = self.get_aggregated_metrics()\n        if aggregated_metrics is None:\n            metrics_plot.clear_figure()\n            metrics_plot.title(\"No run data to show!\")\n            return\n\n        plot_model = self.get_plot_model()\n        if self.show_mode == ShowMode.RunConfiguration:\n            aggregated_metrics = [\n                (run_configuration, metrics)\n                for run_configuration, metrics in aggregated_metrics\n                if run_configuration.name == self.current_run_configuration_name\n            ]\n        if isinstance(plot_model, LinePlotModel):\n            plot_plotext.draw_line_plot(metrics_plot, plot_model, aggregated_metrics)\n        elif isinstance(plot_model, BarChartModel):\n            plot_plotext.draw_bar_chart(metrics_plot, plot_model, aggregated_metrics)\n        elif isinstance(plot_model, RooflinePlotModel):\n            plot_plotext.draw_roofline_plot(\n                metrics_plot, plot_model, aggregated_metrics\n            )\n        metrics_plot_widget.refresh()\n\n    def get_plot_model(\n        self,\n    ) -&gt; LinePlotModel | BarChartModel | RooflinePlotModel | None:\n        \"\"\"Get the model for the current plot.\"\"\"\n        if self.current_plot_index is None or self.current_test_bench is None:\n            return None\n        all_plot_models: list[LinePlotModel | BarChartModel | RooflinePlotModel] = [\n            *self.current_test_bench.bench_model.analysis.line_plots,\n            *self.current_test_bench.bench_model.analysis.bar_charts,\n            *self.current_test_bench.bench_model.analysis.roofline_plots,\n        ]\n        enabled_plot_models = [\n            plot_model for plot_model in all_plot_models if plot_model.enabled\n        ]\n        if len(enabled_plot_models) == 0:\n            return None\n        return enabled_plot_models[self.current_plot_index % len(enabled_plot_models)]\n\n    def action_reload_test_plan(self) -&gt; None:\n        \"\"\"Reload the test plan for the user interface.\"\"\"\n        self.test_plan = TestPlan(\n            self.test_plan.yaml_path, self.test_plan.base_output_directory\n        )\n        self.initialise_test_plan_tree()\n        self.update_all_tabs()\n\n    def action_change_plot(self, offset: int) -&gt; None:\n        \"\"\"Change which plot is being shown in the user interface.\"\"\"\n        if self.current_plot_index is None:\n            self.app.bell()\n            return\n        # TODO: Add support for skipping disabled plots\n        self.current_plot_index += offset\n        self.update_plot_tab()\n\n    def action_open_graph(self) -&gt; None:\n        \"\"\"Open the current plot in matplotlib.\"\"\"\n        if self.current_plot_index is None:\n            self.app.bell()\n            return\n\n        assert self.current_test_bench is not None\n        aggregated_metrics = self.get_aggregated_metrics()\n        if aggregated_metrics is None:\n            return\n\n        plot_model = self.get_plot_model()\n        if self.show_mode == ShowMode.RunConfiguration:\n            aggregated_metrics = [\n                (run_configuration, metrics)\n                for run_configuration, metrics in aggregated_metrics\n                if run_configuration.name == self.current_run_configuration_name\n            ]\n\n        if isinstance(plot_model, LinePlotModel):\n            plot_matplotlib.draw_line_plot(plot_model, aggregated_metrics)\n        elif isinstance(plot_model, BarChartModel):\n            plot_matplotlib.draw_bar_chart(plot_model, aggregated_metrics)\n        elif isinstance(plot_model, RooflinePlotModel):\n            plot_matplotlib.draw_roofline_plot(plot_model, aggregated_metrics)\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.__init__","title":"<code>__init__(test_plan: TestPlan, command_args: Namespace, *args, **kwargs) -&gt; None</code>","text":"<p>Initialise the user interface.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def __init__(  # type: ignore[no-untyped-def]\n    self, test_plan: TestPlan, command_args: Namespace, *args, **kwargs\n) -&gt; None:\n    \"\"\"Initialise the user interface.\"\"\"\n    self.test_plan: TestPlan = test_plan\n    self.command_args: Namespace = command_args\n    self.show_mode = ShowMode.Uninitialised\n    self.current_test_bench: TestBench | None = None\n    self.current_run_configuration: RunConfigurationModel | None = None\n    self.current_run_configuration_name: str | None = None\n    self.current_plot_index: int | None = None\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.compose","title":"<code>compose() -&gt; ComposeResult</code>","text":"<p>Compose the structure of the application.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def compose(self) -&gt; ComposeResult:\n    \"\"\"Compose the structure of the application.\"\"\"\n    yield Header()\n    with Horizontal():\n        # The navigation bar for the test plan\n        with Vertical(id=\"explorer\"):\n            yield Button(\"Run Test Plan\", id=\"run-button\")\n            yield TestPlanTree(label=\"Test Plan\", id=\"tree-explorer\")\n\n        # The starting pane that conceals the data pane when nothing selected\n        with Container(id=\"start-pane\"):\n            yield Label(\"Select a benchmark or run to start\", id=\"start-pane-label\")\n\n        with TabbedContent(initial=INITIAL_TAB, id=\"informer\"):\n            with TabPane(\"Run\", id=\"run-tab\"):\n                yield DataTable(id=\"run-information\")\n                # TODO: Get bash language working\n                yield TextArea(\n                    \"echo hello\",\n                    id=\"sbatch-contents\",\n                    read_only=True,\n                    show_line_numbers=True,\n                    theme=\"monokai\",\n                )\n            with TabPane(\"Metrics\", id=\"metrics-tab\"):\n                yield DataTable(id=\"metrics-table\")\n            with TabPane(\"Plot\", id=\"plot-tab\"):\n                yield PlotextPlot(id=\"metrics-plot\")\n    yield Footer()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.on_mount","title":"<code>on_mount() -&gt; None</code>","text":"<p>Initialise data when the application is created.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def on_mount(self) -&gt; None:\n    \"\"\"Initialise data when the application is created.\"\"\"\n    self.initialise_test_plan_tree()\n    self.query_one(\"#sbatch-contents\", TextArea).register_language(\n        get_language(\"bash\"), BASH_HIGHLIGHTS\n    )\n    self.query_one(\"#sbatch-contents\", TextArea).language = \"bash\"\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.on_button_pressed","title":"<code>on_button_pressed(event: Button.Pressed) -&gt; None</code>","text":"<p>When a button is pressed.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def on_button_pressed(self, event: Button.Pressed) -&gt; None:\n    \"\"\"When a button is pressed.\"\"\"\n    if event.button.id == \"run-button\":\n        self.push_screen(RunDialogScreen())\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.on_data_table_cell_selected","title":"<code>on_data_table_cell_selected(event: DataTable.CellSelected) -&gt; None</code>","text":"<p>When a data table cell is selected.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def on_data_table_cell_selected(self, event: DataTable.CellSelected) -&gt; None:\n    \"\"\"When a data table cell is selected.\"\"\"\n    if event.data_table.id == \"run-information\":\n        self.update_sbatch_contents()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.remove_start_pane","title":"<code>remove_start_pane() -&gt; None</code>","text":"<p>Remove the start pane from the screen if it is uninitialised.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def remove_start_pane(self) -&gt; None:\n    \"\"\"Remove the start pane from the screen if it is uninitialised.\"\"\"\n    if self.show_mode is ShowMode.Uninitialised:\n        self.query_one(\"#start-pane\", Container).remove()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.initialise_test_plan_tree","title":"<code>initialise_test_plan_tree() -&gt; None</code>","text":"<p>Initialise the test plan tree.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def initialise_test_plan_tree(self) -&gt; None:\n    \"\"\"Initialise the test plan tree.\"\"\"\n    tree = self.query_one(TestPlanTree)\n    tree.populate()\n    self.app.set_focus(tree)\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.handle_tree_selection","title":"<code>handle_tree_selection(node: TreeNode[TestPlanTreeType]) -&gt; None</code>","text":"<p>Drive the user interface updates when new tree nodes are selected.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def handle_tree_selection(self, node: TreeNode[TestPlanTreeType]) -&gt; None:\n    \"\"\"Drive the user interface updates when new tree nodes are selected.\"\"\"\n    if node == self.query_one(TestPlanTree).root:\n        return\n\n    self.remove_start_pane()\n\n    if isinstance(node.data, TestBench):\n        self.show_mode = ShowMode.TestBench\n        self.current_test_bench = node.data\n        self.current_run_configuration = None\n        self.current_run_configuration_name = None\n    elif isinstance(node.data, RunConfigurationModel):\n        self.show_mode = ShowMode.RunConfiguration\n        assert node.parent is not None\n        self.current_test_bench = cast(TestBench, node.parent.data)\n        self.current_run_configuration = node.data\n        self.current_run_configuration_name = (\n            str(node.label).strip(\"[dim]\").strip(\"[/dim]\")\n        )\n\n    self.current_plot_index = 0\n    self.update_all_tabs()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.update_all_tabs","title":"<code>update_all_tabs() -&gt; None</code>","text":"<p>Update all tabs in the user interface.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def update_all_tabs(self) -&gt; None:\n    \"\"\"Update all tabs in the user interface.\"\"\"\n    self.update_run_tab()\n    self.update_metrics_tab()\n    self.update_plot_tab()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.update_run_tab","title":"<code>update_run_tab() -&gt; None</code>","text":"<p>Update the run tab of the user interface.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def update_run_tab(self) -&gt; None:\n    \"\"\"Update the run tab of the user interface.\"\"\"\n    self.update_run_information()\n    self.update_sbatch_contents()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.update_run_information","title":"<code>update_run_information() -&gt; None</code>","text":"<p>Update the instantiations table in the run tab.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def update_run_information(self) -&gt; None:\n    \"\"\"Update the instantiations table in the run tab.\"\"\"\n    run_information = self.query_one(\"#run-information\", DataTable)\n\n    assert self.current_test_bench is not None\n    instantiations = self.current_test_bench.instantiations\n\n    run_information.clear(columns=True)\n    if len(instantiations) &gt; 0:\n        run_information.add_columns(*instantiations[0].keys())\n    for instantiation in instantiations:\n        run_information.add_row(*instantiation.values())\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.update_sbatch_contents","title":"<code>update_sbatch_contents() -&gt; None</code>","text":"<p>Update the sbatch contents in the run tab.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def update_sbatch_contents(self) -&gt; None:\n    \"\"\"Update the sbatch contents in the run tab.\"\"\"\n    run_information = self.query_one(\"#run-information\", DataTable)\n    sbatch_contents = self.query_one(\"#sbatch-contents\", TextArea)\n\n    assert self.current_test_bench is not None\n    instantiations = self.current_test_bench.instantiations\n    if self.show_mode == ShowMode.TestBench:\n        sbatch_contents.visible = False\n        sbatch_contents.text = \"\"\n    else:\n        assert self.current_run_configuration is not None\n        assert self.current_run_configuration_name is not None\n        sbatch_contents.visible = True\n        selected_instantiation = instantiations[run_information.cursor_row]\n\n        sbatch_contents.text = self.current_run_configuration.realise(\n            self.current_run_configuration_name,\n            self.current_test_bench.output_directory,\n            selected_instantiation,\n        ).sbatch_contents\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.get_aggregated_metrics","title":"<code>get_aggregated_metrics() -&gt; list[tuple[RunConfiguration, dict[str, str | UFloat]]] | None</code>","text":"<p>.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def get_aggregated_metrics(\n    self,\n) -&gt; list[tuple[RunConfiguration, dict[str, str | UFloat]]] | None:\n    \"\"\".\"\"\"\n    assert self.current_test_bench is not None\n    run_outputs = self.current_test_bench.get_run_outputs()\n    if run_outputs is None:\n        return None\n    run_metrics = self.current_test_bench.get_run_metrics(run_outputs)\n    aggregated_metrics = self.current_test_bench.aggregate_run_metrics(run_metrics)\n    return self.current_test_bench.calculate_derived_metrics(aggregated_metrics)\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.update_metrics_tab","title":"<code>update_metrics_tab() -&gt; None</code>","text":"<p>Update the metrics tab of the user interface.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def update_metrics_tab(self) -&gt; None:\n    \"\"\"Update the metrics tab of the user interface.\"\"\"\n    metrics_table = self.query_one(\"#metrics-table\", DataTable)\n    metrics_table.clear(columns=True)\n\n    assert self.current_test_bench is not None\n    aggregated_metrics = self.get_aggregated_metrics()\n    if aggregated_metrics is None:\n        metrics_table.add_columns(\"No run data to show!\")\n        return\n\n    if self.show_mode == ShowMode.TestBench:\n        metrics_table.add_columns(\n            \"Name\",\n            *list(self.current_test_bench.bench_model.analysis.metrics.keys()),\n            *list(\n                self.current_test_bench.bench_model.analysis.derived_metrics.keys()\n            ),\n        )\n        for run_configuration, metrics in aggregated_metrics:\n            metrics_table.add_row(\n                run_configuration.name,\n                *list(metrics.values()),\n            )\n    else:\n        assert self.current_run_configuration is not None\n        assert self.current_run_configuration_name is not None\n        metrics_table.add_columns(\n            *list(self.current_test_bench.bench_model.analysis.metrics.keys()),\n            *list(\n                self.current_test_bench.bench_model.analysis.derived_metrics.keys()\n            ),\n        )\n        for run_configuration, metrics in aggregated_metrics:\n            if run_configuration.name != self.current_run_configuration_name:\n                continue\n            metrics_table.add_row(*list(metrics.values()))\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.update_plot_tab","title":"<code>update_plot_tab() -&gt; None</code>","text":"<p>Update the plot tab of the user interface.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def update_plot_tab(self) -&gt; None:\n    \"\"\"Update the plot tab of the user interface.\"\"\"\n    metrics_plot_widget = self.query_one(\"#metrics-plot\", PlotextPlot)\n    metrics_plot = metrics_plot_widget.plt\n\n    assert self.current_test_bench is not None\n    aggregated_metrics = self.get_aggregated_metrics()\n    if aggregated_metrics is None:\n        metrics_plot.clear_figure()\n        metrics_plot.title(\"No run data to show!\")\n        return\n\n    plot_model = self.get_plot_model()\n    if self.show_mode == ShowMode.RunConfiguration:\n        aggregated_metrics = [\n            (run_configuration, metrics)\n            for run_configuration, metrics in aggregated_metrics\n            if run_configuration.name == self.current_run_configuration_name\n        ]\n    if isinstance(plot_model, LinePlotModel):\n        plot_plotext.draw_line_plot(metrics_plot, plot_model, aggregated_metrics)\n    elif isinstance(plot_model, BarChartModel):\n        plot_plotext.draw_bar_chart(metrics_plot, plot_model, aggregated_metrics)\n    elif isinstance(plot_model, RooflinePlotModel):\n        plot_plotext.draw_roofline_plot(\n            metrics_plot, plot_model, aggregated_metrics\n        )\n    metrics_plot_widget.refresh()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.get_plot_model","title":"<code>get_plot_model() -&gt; LinePlotModel | BarChartModel | RooflinePlotModel | None</code>","text":"<p>Get the model for the current plot.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def get_plot_model(\n    self,\n) -&gt; LinePlotModel | BarChartModel | RooflinePlotModel | None:\n    \"\"\"Get the model for the current plot.\"\"\"\n    if self.current_plot_index is None or self.current_test_bench is None:\n        return None\n    all_plot_models: list[LinePlotModel | BarChartModel | RooflinePlotModel] = [\n        *self.current_test_bench.bench_model.analysis.line_plots,\n        *self.current_test_bench.bench_model.analysis.bar_charts,\n        *self.current_test_bench.bench_model.analysis.roofline_plots,\n    ]\n    enabled_plot_models = [\n        plot_model for plot_model in all_plot_models if plot_model.enabled\n    ]\n    if len(enabled_plot_models) == 0:\n        return None\n    return enabled_plot_models[self.current_plot_index % len(enabled_plot_models)]\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.action_reload_test_plan","title":"<code>action_reload_test_plan() -&gt; None</code>","text":"<p>Reload the test plan for the user interface.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def action_reload_test_plan(self) -&gt; None:\n    \"\"\"Reload the test plan for the user interface.\"\"\"\n    self.test_plan = TestPlan(\n        self.test_plan.yaml_path, self.test_plan.base_output_directory\n    )\n    self.initialise_test_plan_tree()\n    self.update_all_tabs()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.action_change_plot","title":"<code>action_change_plot(offset: int) -&gt; None</code>","text":"<p>Change which plot is being shown in the user interface.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def action_change_plot(self, offset: int) -&gt; None:\n    \"\"\"Change which plot is being shown in the user interface.\"\"\"\n    if self.current_plot_index is None:\n        self.app.bell()\n        return\n    # TODO: Add support for skipping disabled plots\n    self.current_plot_index += offset\n    self.update_plot_tab()\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.interactive_ui.UserInterface.action_open_graph","title":"<code>action_open_graph() -&gt; None</code>","text":"<p>Open the current plot in matplotlib.</p> Source code in <code>src/hpc_multibench/tui/interactive_ui.py</code> <pre><code>def action_open_graph(self) -&gt; None:\n    \"\"\"Open the current plot in matplotlib.\"\"\"\n    if self.current_plot_index is None:\n        self.app.bell()\n        return\n\n    assert self.current_test_bench is not None\n    aggregated_metrics = self.get_aggregated_metrics()\n    if aggregated_metrics is None:\n        return\n\n    plot_model = self.get_plot_model()\n    if self.show_mode == ShowMode.RunConfiguration:\n        aggregated_metrics = [\n            (run_configuration, metrics)\n            for run_configuration, metrics in aggregated_metrics\n            if run_configuration.name == self.current_run_configuration_name\n        ]\n\n    if isinstance(plot_model, LinePlotModel):\n        plot_matplotlib.draw_line_plot(plot_model, aggregated_metrics)\n    elif isinstance(plot_model, BarChartModel):\n        plot_matplotlib.draw_bar_chart(plot_model, aggregated_metrics)\n    elif isinstance(plot_model, RooflinePlotModel):\n        plot_matplotlib.draw_roofline_plot(plot_model, aggregated_metrics)\n</code></pre>"},{"location":"reference/tui.html#hpc_multibench.tui.bash_highlights","title":"<code>bash_highlights</code>","text":"<p>A tree-sitter query for highlighting bash.</p> <p>Query string derived from: https://github.com/nvim-treesitter/nvim-treesitter/blob/f95ffd09ed35880c3a46ad2b968df361fa592a76/queries/bash/highlights.scm</p> <p>Which is aligned with the approach taken for Textual builtin languages, for example: https://github.com/Textualize/textual/blob/main/src/textual/tree-sitter/highlights/python.scm</p> <p>The Nvim treesitter library from which this is derived is licensed under the Apache 2.0 license, which allows modification and distribution for both public and private use. The original version of the grammar used was written by TravonteD on GitHub.</p>"},{"location":"reference/yaml_model.html","title":"Reference","text":""},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model","title":"<code>yaml_model</code>","text":"<p>A set of objects modelling the YAML schema for a test plan.</p>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.RunConfigurationModel","title":"<code>RunConfigurationModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for an executable.</p> Source code in <code>src/hpc_multibench/yaml_model.py</code> <pre><code>class RunConfigurationModel(BaseModel):\n    \"\"\"A Pydantic model for an executable.\"\"\"\n\n    sbatch_config: dict[str, Any]\n    module_loads: list[str]\n    environment_variables: dict[str, Any]\n    directory: Path\n    build_commands: list[str]\n    run_command: str\n    args: str | None = None\n    post_commands: list[str] = []\n\n    def realise(\n        self,\n        name: str,\n        output_directory: Path,\n        instantiation: dict[str, Any] | None,\n    ) -&gt; RunConfiguration:\n        \"\"\"Construct a run configuration from its data model.\"\"\"\n        run = RunConfiguration(name, self.run_command, output_directory)\n        run.sbatch_config = self.sbatch_config\n        run.module_loads = self.module_loads\n        run.environment_variables = self.environment_variables\n        run.directory = Path(self.directory)\n        run.build_commands = self.build_commands\n        run.post_commands = self.post_commands\n        run.args = self.args\n        run.instantiation = instantiation\n\n        # Update the run configuration based on the instantiation\n        if instantiation is not None:\n            for key, value in instantiation.items():\n                # TODO: Error checking on keys\n                if key == \"sbatch_config\":\n                    run.sbatch_config.update(value)\n                # TODO: Further root cause why this was causing duplicate runs\n                # elif key == \"environment_variables\":\n                #     run.environment_variables.update(value)\n                else:\n                    setattr(run, key, value)\n\n        return run\n</code></pre>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.RunConfigurationModel.realise","title":"<code>realise(name: str, output_directory: Path, instantiation: dict[str, Any] | None) -&gt; RunConfiguration</code>","text":"<p>Construct a run configuration from its data model.</p> Source code in <code>src/hpc_multibench/yaml_model.py</code> <pre><code>def realise(\n    self,\n    name: str,\n    output_directory: Path,\n    instantiation: dict[str, Any] | None,\n) -&gt; RunConfiguration:\n    \"\"\"Construct a run configuration from its data model.\"\"\"\n    run = RunConfiguration(name, self.run_command, output_directory)\n    run.sbatch_config = self.sbatch_config\n    run.module_loads = self.module_loads\n    run.environment_variables = self.environment_variables\n    run.directory = Path(self.directory)\n    run.build_commands = self.build_commands\n    run.post_commands = self.post_commands\n    run.args = self.args\n    run.instantiation = instantiation\n\n    # Update the run configuration based on the instantiation\n    if instantiation is not None:\n        for key, value in instantiation.items():\n            # TODO: Error checking on keys\n            if key == \"sbatch_config\":\n                run.sbatch_config.update(value)\n            # TODO: Further root cause why this was causing duplicate runs\n            # elif key == \"environment_variables\":\n            #     run.environment_variables.update(value)\n            else:\n                setattr(run, key, value)\n\n    return run\n</code></pre>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.LinePlotModel","title":"<code>LinePlotModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for a line plot of two variables.</p> Source code in <code>src/hpc_multibench/yaml_model.py</code> <pre><code>class LinePlotModel(BaseModel):\n    \"\"\"A Pydantic model for a line plot of two variables.\"\"\"\n\n    title: str\n    x: str\n    y: str | list[str]\n    split_metrics: list[str] = []\n    fix_metrics: dict[str, Any] = {}\n    x_log: bool = False\n    y_log: bool = False\n    x_lim: float | tuple[float, float] | None = None\n    y_lim: float | tuple[float, float] | None = None\n    enabled: bool = True\n</code></pre>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.BarChartModel","title":"<code>BarChartModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for a bar chart of a single variable.</p> Source code in <code>src/hpc_multibench/yaml_model.py</code> <pre><code>class BarChartModel(BaseModel):\n    \"\"\"A Pydantic model for a bar chart of a single variable.\"\"\"\n\n    title: str\n    y: str | list[str]\n    split_metrics: list[str] = []\n    fix_metrics: dict[str, Any] = {}\n    y_log: bool = False\n    y_lim: float | tuple[float, float] | None = None\n    enabled: bool = True\n</code></pre>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.RooflinePlotModel","title":"<code>RooflinePlotModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for a roofline plot from two metrics.</p> Source code in <code>src/hpc_multibench/yaml_model.py</code> <pre><code>class RooflinePlotModel(BaseModel):\n    \"\"\"A Pydantic model for a roofline plot from two metrics.\"\"\"\n\n    title: str\n    gflops_per_sec: str\n    mbytes_per_sec: str\n    ert_json: Path\n    enabled: bool = True\n</code></pre>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.ExportModel","title":"<code>ExportModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for a exporting a set of metrics.</p> Source code in <code>src/hpc_multibench/yaml_model.py</code> <pre><code>class ExportModel(BaseModel):\n    \"\"\"A Pydantic model for a exporting a set of metrics.\"\"\"\n\n    export_path: Path | None\n    export_format: str = \"csv\"\n    # metrics: list[str] | None = None\n    enabled: bool = True\n</code></pre>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.AnalysisModel","title":"<code>AnalysisModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for a test bench\u2019s analysis operations.</p> Source code in <code>src/hpc_multibench/yaml_model.py</code> <pre><code>class AnalysisModel(BaseModel):\n    \"\"\"A Pydantic model for a test bench's analysis operations.\"\"\"\n\n    metrics: dict[str, str]\n    derived_metrics: dict[str, str] = {}\n    line_plots: list[LinePlotModel] = []\n    bar_charts: list[BarChartModel] = []\n    roofline_plots: list[RooflinePlotModel] = []\n    data_exports: list[ExportModel] = []\n</code></pre>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.RerunModel","title":"<code>RerunModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for the test bench\u2019s statistical re-runs.</p> Source code in <code>src/hpc_multibench/yaml_model.py</code> <pre><code>class RerunModel(BaseModel):\n    \"\"\"A Pydantic model for the test bench's statistical re-runs.\"\"\"\n\n    number: int\n    highest_discard: int = 0\n    lowest_discard: int = 0\n    unaggregatable_metrics: list[str] = []\n\n    @property\n    def undiscarded_number(self) -&gt; int:\n        \"\"\"Return the number of undiscarded reruns.\"\"\"\n        return self.number - self.highest_discard - self.lowest_discard\n</code></pre>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.RerunModel.undiscarded_number","title":"<code>undiscarded_number: int</code>  <code>property</code>","text":"<p>Return the number of undiscarded reruns.</p>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.BenchModel","title":"<code>BenchModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for a test bench.</p> Source code in <code>src/hpc_multibench/yaml_model.py</code> <pre><code>class BenchModel(BaseModel):\n    \"\"\"A Pydantic model for a test bench.\"\"\"\n\n    run_configurations: list[str]\n    matrix: dict[str | tuple[str, ...], list[Any]]\n    analysis: AnalysisModel\n    reruns: RerunModel = RerunModel(number=1)\n    enabled: bool = True\n</code></pre>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.TestPlanModel","title":"<code>TestPlanModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for a set of test benches and their executables.</p> Source code in <code>src/hpc_multibench/yaml_model.py</code> <pre><code>class TestPlanModel(BaseModel):\n    \"\"\"A Pydantic model for a set of test benches and their executables.\"\"\"\n\n    run_configurations: dict[str, RunConfigurationModel]\n    benches: dict[str, BenchModel]\n\n    @classmethod\n    def from_yaml(cls, file: Path) -&gt; Self:\n        \"\"\"Construct the model from a YAML file.\"\"\"\n        with file.open(encoding=\"utf-8\") as handle:\n            return cls(**YAML(typ=\"safe\").load(handle))\n</code></pre>"},{"location":"reference/yaml_model.html#hpc_multibench.yaml_model.TestPlanModel.from_yaml","title":"<code>from_yaml(file: Path) -&gt; Self</code>  <code>classmethod</code>","text":"<p>Construct the model from a YAML file.</p> Source code in <code>src/hpc_multibench/yaml_model.py</code> <pre><code>@classmethod\ndef from_yaml(cls, file: Path) -&gt; Self:\n    \"\"\"Construct the model from a YAML file.\"\"\"\n    with file.open(encoding=\"utf-8\") as handle:\n        return cls(**YAML(typ=\"safe\").load(handle))\n</code></pre>"}]}